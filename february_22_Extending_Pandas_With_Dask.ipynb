{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "kernel_info": {
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.8",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "nteract": {
      "version": "0.15.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rcameronc/meetings/blob/main/february_22_Extending_Pandas_With_Dask.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<td> <img src=\"https://sdzwildlifeexplorers.org/sites/default/files/2017-07/pandas-closeup.jpg\" alt=\"Drawing\" style=\"width: 250px;\"/> </td>\n",
        "\n",
        "## Extending Pandas with Dask\n",
        "\n",
        "Feb 22rd, 2023\n",
        "\n",
        "by [Roger Creel](https://rogercreel.com) for the [Columbia Data Club](https://github.com/columbia-data-club/).\n",
        "\n",
        "This notebook underpins a ~60-75 minute presentation that demonstrates how to use Pandas with Dask.  It is geared towards complete beginners to Python and to programming. \n"
      ],
      "metadata": {
        "id": "pabZUd_Cj5xq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swftxqBHPEC_"
      },
      "source": [
        "# **Extending Pandas with Dask**\n",
        "*Presented by Columbia University Libraries*\n",
        "***\n",
        "\n",
        "Welcome to the Columbia University Library's Estending Pandas with Dask course! These are our objectives:\n",
        "\n",
        "* Review Pandas as a tool for data analysis\n",
        "* Explore how to use Dask dataframes as an extension for Pandas\n",
        "* Discuss statistics and visualization capabilities of Dask + Pandas\n",
        "* Answer questions! \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MB2Yn8LqTheu"
      },
      "source": [
        "## **Getting started**\n",
        "### *(with Google Colab)*\n",
        "\n",
        "Topics to be covered:\n",
        "1. What is Python?\n",
        "2. Why does it matter?\n",
        "3. How can you use Python? (IDEs, notebooks, terminal, Colab)\n",
        "4. What are packages and why do we need them?\n",
        "5. Basic familiarity with CoLab (shareability, power)\n",
        "6. Pitfalls of using CoLab\n",
        "7. Why pandas? Pandas is an open-source library that provides high-performance, easy-to-use data structures and data analysis tools for the Python programming language.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run these once per session, then no need to rerun them again even if kernel dies\n",
        "!python -m pip install 'fastparquet'\n",
        "!python -m pip install \"dask[complete]\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQogGX6O1rWU",
        "outputId": "a29e615b-d036-4af2-fe6e-6129bc107c4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fastparquet\n",
            "  Downloading fastparquet-2023.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pandas>=1.5.0\n",
            "  Downloading pandas-1.5.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.8/dist-packages (from fastparquet) (1.22.4)\n",
            "Collecting cramjam>=2.3\n",
            "  Downloading cramjam-2.6.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from fastparquet) (23.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.8/dist-packages (from fastparquet) (2023.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.5.0->fastparquet) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.5.0->fastparquet) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.8.1->pandas>=1.5.0->fastparquet) (1.15.0)\n",
            "Installing collected packages: cramjam, pandas, fastparquet\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.3.5\n",
            "    Uninstalling pandas-1.3.5:\n",
            "      Successfully uninstalled pandas-1.3.5\n",
            "Successfully installed cramjam-2.6.2 fastparquet-2023.2.0 pandas-1.5.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: dask[complete] in /usr/local/lib/python3.8/dist-packages (2022.2.1)\n",
            "Requirement already satisfied: partd>=0.3.10 in /usr/local/lib/python3.8/dist-packages (from dask[complete]) (1.3.0)\n",
            "Requirement already satisfied: toolz>=0.8.2 in /usr/local/lib/python3.8/dist-packages (from dask[complete]) (0.12.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.8/dist-packages (from dask[complete]) (6.0)\n",
            "Requirement already satisfied: fsspec>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from dask[complete]) (2023.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from dask[complete]) (23.0)\n",
            "Requirement already satisfied: cloudpickle>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from dask[complete]) (2.2.1)\n",
            "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.8/dist-packages (from dask[complete]) (1.5.3)\n",
            "Requirement already satisfied: distributed==2022.02.1 in /usr/local/lib/python3.8/dist-packages (from dask[complete]) (2022.2.1)\n",
            "Requirement already satisfied: bokeh>=2.1.1 in /usr/local/lib/python3.8/dist-packages (from dask[complete]) (2.3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from dask[complete]) (2.11.3)\n",
            "Requirement already satisfied: numpy>=1.18 in /usr/local/lib/python3.8/dist-packages (from dask[complete]) (1.22.4)\n",
            "Requirement already satisfied: msgpack>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from distributed==2022.02.1->dask[complete]) (1.0.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from distributed==2022.02.1->dask[complete]) (57.4.0)\n",
            "Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.8/dist-packages (from distributed==2022.02.1->dask[complete]) (2.4.0)\n",
            "Requirement already satisfied: click>=6.6 in /usr/local/lib/python3.8/dist-packages (from distributed==2022.02.1->dask[complete]) (7.1.2)\n",
            "Requirement already satisfied: zict>=0.1.3 in /usr/local/lib/python3.8/dist-packages (from distributed==2022.02.1->dask[complete]) (2.2.0)\n",
            "Requirement already satisfied: tornado>=6.0.3 in /usr/local/lib/python3.8/dist-packages (from distributed==2022.02.1->dask[complete]) (6.2)\n",
            "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from distributed==2022.02.1->dask[complete]) (1.7.0)\n",
            "Requirement already satisfied: psutil>=5.0 in /usr/local/lib/python3.8/dist-packages (from distributed==2022.02.1->dask[complete]) (5.4.8)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from bokeh>=2.1.1->dask[complete]) (2.8.2)\n",
            "Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.8/dist-packages (from bokeh>=2.1.1->dask[complete]) (7.1.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.8/dist-packages (from bokeh>=2.1.1->dask[complete]) (4.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->dask[complete]) (2.0.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.0->dask[complete]) (2022.7.1)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.8/dist-packages (from partd>=0.3.10->dask[complete]) (1.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->bokeh>=2.1.1->dask[complete]) (1.15.0)\n",
            "Requirement already satisfied: heapdict in /usr/local/lib/python3.8/dist-packages (from zict>=0.1.3->distributed==2022.02.1->dask[complete]) (1.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9ry0bDNm6MP"
      },
      "source": [
        "# Import packages\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "import statsmodels as sm\n",
        "import requests\n",
        "import pyarrow\n",
        "import os\n",
        "import numpy as np\n",
        "import dask\n",
        "import dask.dataframe as dd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll first download three years of data from the New York Taxi & Limousine Commission (TLC) [Trip Record Data website](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page). "
      ],
      "metadata": {
        "id": "rtE42ThFqI0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a directory for our data if it doesn't exist\n",
        "path = f'./taxi_data_parquet/'\n",
        "if not os.path.isdir(path):\n",
        "  os.makedirs(path)\n",
        "\n",
        "months = [str(i).zfill(2) for i in range(1,13)]\n",
        "years = ['2019', '2020']\n",
        "# get the URL of one month's data from New York Suway \n",
        "for year in years:\n",
        "  print(year)\n",
        "  for month in months:\n",
        "    print(month)\n",
        "    url = f\"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_{year}-{month}.parquet\"\n",
        "\n",
        "    r = requests.get(url) # create HTTP response object\n",
        "    with open(path + f\"yellow_tripdata_{year}-{month}.parquet\",'wb') as f:\n",
        "\n",
        "          f.write(r.content)\n",
        "\n",
        "# save parquet files as csv files \n",
        "\n",
        "# open parquet file\n",
        "df = pd.read_parquet(path + 'yellow_tripdata_2020-01.parquet')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "waD17YJflsgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We just downloaded a lot of files locally in the format TLC gave it to us: an Apache `parquet` file.  Let's check how much data we have using shell scripting command `du -sh` (disk usage, total size, human readable):"
      ],
      "metadata": {
        "id": "eSLNTKwAjoYR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cd taxi_data_parquet\n",
        "\n",
        "du -sh"
      ],
      "metadata": {
        "id": "C36SC6n5jtoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How many files do we have? Let's find out using the `find` and `wc -l` (word cound by line) commands, connected via a `|` pipe: "
      ],
      "metadata": {
        "id": "352J-40cj_WM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cd taxi_data_parquet\n",
        "find . | wc -l"
      ],
      "metadata": {
        "id": "aqs183p-j_FW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We don't want to load all those files separately.  That would be a nightmare! We want to work with them all at once.  Dask will makes that easy.  First let's get our filenames:"
      ],
      "metadata": {
        "id": "ckeqKQrwkOfO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from glob import glob\n",
        "filenames = sorted(glob('taxi_data_parquet/yellow_tripdata*.parquet'))\n",
        "print(filenames[:3])\n",
        "print(filenames[-3:])\n"
      ],
      "metadata": {
        "id": "p3X9Dq13kchT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's open one of them with pandas to remind ourselves what they look like."
      ],
      "metadata": {
        "id": "vAcvTEZlmQOe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_one = pd.read_parquet(filenames[0])\n",
        "df_one"
      ],
      "metadata": {
        "id": "GqWAwnVTmTHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "More than seven million entries!  That's a lot, and only for one month.  If we want to look at three years of data, that would be 100+ million taxi rides.  Too many for to load into memory and do computations on.  Thankfully, Dask Dataframes is going to come to our rescue.  "
      ],
      "metadata": {
        "id": "si_yrOcymz3q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we'll use just the first half of 2020 because we have limited RAM\n",
        "filenames_fewer = filenames[12:-9]\n",
        "\n",
        "df = dd.read_parquet(filenames_fewer,\n",
        "                     filename_suffix=\".parquet\",\n",
        "                     engine='fastparquet',\n",
        "                     parse_dates=[\n",
        "                         \"tpep_pickup_datetime\",\t\"tpep_dropoff_datetime\"\n",
        "                         ],  \n",
        "                    #  dtypes=df.dtypes.to_dict(),\n",
        "                     )\n",
        "df"
      ],
      "metadata": {
        "id": "x06x7LZll-_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_orig_len = len(df)\n",
        "df_orig_len"
      ],
      "metadata": {
        "id": "Gm3bITNXtVEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What do you see? Let's look at the first rows to check they are the same."
      ],
      "metadata": {
        "id": "qKhKxAX0mMIx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Get the first 5 rows\n",
        "df.head(5)"
      ],
      "metadata": {
        "id": "YkmRa0kdmohF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These columns have long, hard-to-read names.  We'll want to make them shorter and drop the less useful ones."
      ],
      "metadata": {
        "id": "gRCPrQ8ztk_s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Refresher on Python data structures**"
      ],
      "metadata": {
        "id": "9aN18WFDb5v-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "But before we do that, let's review a few key data structures in python and pandas:"
      ],
      "metadata": {
        "id": "g6Ei3xEpzT2p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lists\n",
        "listex = [1, 2, 3, 4, \"python\", \"makes\", \"rory\", \"roar\"]\n",
        "listex[1]\n",
        "listex[0:5]\n",
        "\n",
        "# Iterable"
      ],
      "metadata": {
        "id": "mheBjzqpzYY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionaries\n",
        "dictex = {\"rory\" : \"the lion\", \"columbia\": \"the university\", \"founded\": 1754}\n",
        "dictex[\"founded\"]\n",
        "dictex[\"rory\"]\n",
        "\n",
        "# Iterable"
      ],
      "metadata": {
        "id": "uDCBw_l3zfXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's clean up our data.  "
      ],
      "metadata": {
        "id": "cJULoTlN0fEV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "P_0tCqq2t9Q5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We don't need all the columns, and the columns we do need have cumbersome names.  Let's fix that. "
      ],
      "metadata": {
        "id": "KnETcGBXcDCk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns = {\n",
        "          'tpep_pickup_datetime':'pickup',\n",
        "          'tpep_dropoff_datetime':'dropoff',\n",
        "          'passenger_count':'passengers', \n",
        "          'trip_distance':'distance', \n",
        "          'payment_type':'payment_type',\n",
        "          'fare_amount':'fare',\n",
        "          'extra':'extra',\n",
        "          'mta_tax':'tax', \n",
        "          'tip_amount':'tip', \n",
        "          'tolls_amount':'tolls', \n",
        "          'improvement_surcharge':'improvement_surcharge',\n",
        "          'total_amount':'total_fare', \n",
        "          'congestion_surcharge':'congestion_tax', \n",
        "          'airport_fee':'airport_fee'\n",
        "          }\n",
        "\n",
        "# choose only columns that are keys in dictionary\n",
        "df = df[list(columns.keys())]\n",
        "\n",
        "# rename columns by values of dictionary\n",
        "df = df.rename(columns=columns)\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "0ELXh_OBuA-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pandas series\n",
        "type(df[\"pickup\"])\n",
        "\n",
        "# Column"
      ],
      "metadata": {
        "id": "5WG3lTNE0yJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.dtypes"
      ],
      "metadata": {
        "id": "DLtorzJHzjj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4uCYWd9pxrR"
      },
      "source": [
        "# Descriptions\n",
        "df.describe(include = \"all\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Any ideas about why it broke?  "
      ],
      "metadata": {
        "id": "CqLImxWNtktC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe(include=['float64'])"
      ],
      "metadata": {
        "id": "YZHZSmA8z02K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can run this, but look at the number of tasks.  What does this mean?  "
      ],
      "metadata": {
        "id": "gnsuGU0itsML"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Task graph](https://www.odatis-ocean.fr/fileadmin/_processed_/a/8/csm_Pangeo_Dask_calcul_parallele_scheduler_1ae2fe84d9.png)"
      ],
      "metadata": {
        "id": "x0xBblD2t8Ep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.iloc[:,0].visualize()"
      ],
      "metadata": {
        "id": "P3Q0JmHvuHCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_passengers = df.passengers.mean()\n",
        "mean_passengers.visualize() # .persist()\n",
        "# mean_pass.compute()"
      ],
      "metadata": {
        "id": "isZxExJm01mF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_pass_inmem = mean_passengers.persist()\n",
        "mean_pass_inmem.visualize()"
      ],
      "metadata": {
        "id": "XWNdFvE2vJwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_pass_inmem.compute()"
      ],
      "metadata": {
        "id": "IVroZCAov0S8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dask has some fast operations and some slower ones. Here are the fastest:\n",
        "\n",
        "Element-wise operations: `df.x + df.y`, `df * df`\n",
        "\n",
        "Row-wise selections: `df[df.x > 0]`\n",
        "\n",
        "Loc: `df.loc[4.0:10.5]`\n",
        "\n",
        "Common aggregations: `df.x.max(), df.max()`\n",
        "\n",
        "Is in: `df[df.x.isin([1, 2, 3])]`\n",
        "\n",
        "Date time/string accessors: `df.timestamp.month`"
      ],
      "metadata": {
        "id": "doQjXxocxMYx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ridetime = (df.dropoff - df.pickup)\n",
        "ridetime"
      ],
      "metadata": {
        "id": "Xa-2RCoLw3t0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The operation was fast, but running the tasks still takes time. "
      ],
      "metadata": {
        "id": "emvVIjaRxxyk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ridetime = ridetime.loc[:10].persist()\n",
        "ridetime.compute()"
      ],
      "metadata": {
        "id": "yLaEOeYjxwhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Other fast-ish (Cleverly parallelizable) tasks:\n",
        "\n",
        "\n",
        " operations (fast):\n",
        "groupby-aggregate (with common aggregations): `df.groupby(df.x).y.max()`, `df.groupby('x').min()` (see Aggregate)\n",
        "\n",
        "groupby-apply on index: `df.groupby(['idx', 'x']).apply(myfunc)`, where idx is the index level name\n",
        "\n",
        "value_counts: `df.x.value_counts()`\n",
        "\n",
        "Drop duplicates: `df.x.drop_duplicates()`\n",
        "\n",
        "Join on index: `dd.merge(df1, df2, left_index=True, right_index=True)` or `dd.merge(df1, df2, on=['idx', 'x'])` where idx is the index name for both df1 and df2\n",
        "\n",
        "Join with pandas DataFrames: `dd.merge(df1, df2, on='id')`\n",
        "\n",
        "Element-wise operations with different partitions / divisions: `df1.x + df2.y`\n",
        "\n",
        "Date time resampling: `df.resample(...)`\n",
        "\n",
        "Rolling averages: `df.rolling(...)`\n",
        "\n",
        "Pearson’s correlation: `df[['col1', 'col2']].corr()`"
      ],
      "metadata": {
        "id": "9nBdIg5wx7lm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"pickup_day\"] = df[\"pickup\"].dt.day\n",
        "df[\"pickup_year\"] = df[\"pickup\"].dt.year\n",
        "df[\"pickup_month\"] = df[\"pickup\"].dt.month"
      ],
      "metadata": {
        "id": "GDwjR-xGWO0e"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78kTBEk9UxXR"
      },
      "source": [
        "# Start with a simple histogram\n",
        "\n",
        "# First with months\n",
        "df_mnth_cnt = df.groupby(\"pickup_day\")[[\"pickup\"]].count().persist()\n",
        "df_mnth_cnt.compute().plot(kind='bar')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[['distance', 'fare']].corr().compute()"
      ],
      "metadata": {
        "id": "Qn3my_2pB49l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why would this correlation be so low?  "
      ],
      "metadata": {
        "id": "odKwse0oC9NA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[df['pickup_year'] > 2018]\n",
        "df = df[(df['fare'] > 2.50) & (df['fare'] < 500)]\n",
        "df = df[(df['distance'] > 0.05) & (df['distance'] < 500)]\n",
        "df = df[df[\"passengers\"] > 0]"
      ],
      "metadata": {
        "id": "TcI4DWXJDARU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Did dropping these bad values improve correlations between, for instance, distance and fare?\n"
      ],
      "metadata": {
        "id": "ta-KAicSDEkf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[['distance', 'fare']].corr().compute()"
      ],
      "metadata": {
        "id": "5gig-qohDHKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df[['passengers', 'distance']].corr().compute()"
      ],
      "metadata": {
        "id": "BzvR3U-lDJjE"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Other operations are slow, however, because they require a shuffle:\n",
        "\n",
        "Set index: `df.set_index(df.x)`\n",
        "\n",
        "groupby-apply not on index (with anything): `df.groupby(df.x).apply(myfunc)`\n",
        "\n",
        "Join not on the index: `dd.merge(df1, df2, on='name')`"
      ],
      "metadata": {
        "id": "j5Qbh66-zUaD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why is shuffling hard?  "
      ],
      "metadata": {
        "id": "XW0AuWLTzrl2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![shuffle graphic](https://assets-global.website-files.com/63192998e5cab906c1b55f6e/633f7b5df9c63728c2ce7ac6_image-3-700x340.png)"
      ],
      "metadata": {
        "id": "ftlcTv7DztfY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Every output partition depends on every input partition, so the graph becomes N² in size. Even with reasonable amounts of input data, this can crash the Dask scheduler."
      ],
      "metadata": {
        "id": "orHSkGQbz2UI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![crazy graph](https://assets-global.website-files.com/63192998e5cab906c1b55f6e/633f7b5df9c6372f4bce7ac3_image7.png)\n",
        "The current task graph of a very small shuffle (20 partitions). It grows quadratically with the number of partitions, so imagine this times 100 or 1000—it gets large very quickly!\n"
      ],
      "metadata": {
        "id": "9O0BvkOFz5Rt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Asking data-driven questions\n",
        "\n",
        "Now that we have the data, we can ask questions with it.  For instance:\n",
        "\n",
        "* How did average number of passengers change during the pandemic?\n",
        "* Did pandemic drivers get tipped more?\n",
        "* Did COVID change ride durations or distances? "
      ],
      "metadata": {
        "id": "Bxj3cG2B0qQn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_cov = df[df['pickup_month'] >= 4]\n",
        "df_precov = df[df['pickup_month'] == 2]\n"
      ],
      "metadata": {
        "id": "fQK0dgCl2_hQ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cov['tip'].mean().compute()"
      ],
      "metadata": {
        "id": "sZ3XrG8_bCe1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_precov['tip'].mean().compute()"
      ],
      "metadata": {
        "id": "xOoWq6urdOHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cov['tip'].std().compute()"
      ],
      "metadata": {
        "id": "eRfIvmXSjnto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_precov['tip'].std().compute()"
      ],
      "metadata": {
        "id": "U8BM3OI4jpya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(df_precov), len(df_cov))"
      ],
      "metadata": {
        "id": "61M82dnkj6j6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaqyKzwNqa95"
      },
      "source": [
        "There are many other fun queestions we could ask:\n",
        "\n",
        "- investigate the potential effects of increasing the number of people in a taxi. Does it affect how likely and how much someone is to tip? Does it relate to how far they travel?\n",
        "- how much do people generally tip?\n",
        "- Are there differences in volume of passengers during different times of day? \n",
        "- What about payment type - who is still using cash, and at what time of day? Are they groups?\n"
      ]
    }
  ]
}