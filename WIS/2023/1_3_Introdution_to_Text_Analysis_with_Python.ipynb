{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "fLyLN6_jA-oW",
        "eo1JZBbtbsfT",
        "rF9J9dgzci0k",
        "qJOpBrywd7Yx",
        "2VVyvGpYl2Wh",
        "_FVfSMb_u6uW",
        "r9gCQVBztyot",
        "V91F0EFIuAQB",
        "iv4TEghp4USS"
      ],
      "mount_file_id": "1htHyzP5ThjyAk4VOOaAmsm7vHob2MxEr",
      "authorship_tag": "ABX9TyM+J85lsmjwv73iRnIKrg3A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/columbia-data-club/meetings/blob/main/WIS/2023/1_3_Introdution_to_Text_Analysis_with_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![A blue background with the Python logo and the words Data Club on it](https://raw.githubusercontent.com/columbia-data-club/meetings/main/assets/images/data-club-python.png)\n",
        "\n",
        "## Introduction to Text Analysis with Python\n",
        "\n",
        "Dec. 1, 2023\n",
        "\n",
        "by [Moacir P. de Sá Pereira](https://moacir.com) for [Women in STEM @ SIPA](https://sipa.campusgroups.com/wis/home/), modified from  [Columbia Data Club](https://github.com/columbia-data-club/) notebooks.\n",
        "\n",
        "This notebook underpins a ~75 minute presentation that uses a corpus of articles downloaded from [JSTOR](http://www.jstor.org) as an occasion to learn the basics of text analysis for complete beginners to Python and to programming.\n",
        "\n",
        "Though some text analysis libraries are mentioned, we will actually limit ourselves becaose of time constraints to more straightforward data analysis."
      ],
      "metadata": {
        "id": "RCGXsCfDq3ic"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Determining Gender of Authors"
      ],
      "metadata": {
        "id": "fLyLN6_jA-oW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we move to NLP of the sort requiring the libraries above, Vanshika suggested we see if we can intuit anything from our _JPE_ dataset regarding female authors.\n",
        "\n",
        "Given what we developed in the previous module, how can we go about seeing how _JPE_ has historically published women? What do we need to do?"
      ],
      "metadata": {
        "id": "Dceu9FgSBDq_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Plan\n"
      ],
      "metadata": {
        "id": "eo1JZBbtbsfT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given a list of articles, here is a step-by-step description of how we can solve our problem:\n",
        "\n",
        "1. We load in the data!\n",
        "\n",
        "1. We turn our list of articles into a list of authors.\n",
        "\n",
        "1. We ascertain the gender of each author\n",
        "\n",
        "1. We match our name-to-gender lookup table to the original list of articles so we can see which articles have women authors\n",
        "\n",
        "1. We group our dataset of articles by whatever criteria we like to see the relationship between the number? percentage? of women authors and whatever criteria we are tracking."
      ],
      "metadata": {
        "id": "l5oj1wM7bxUV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. We Load in the Data!\n",
        "\n",
        "This should be straight forward. We saved the data in the last module as “drive/MyDrive/jpt-full-no-0-authors.parquet” in our Google Drive, so let’s mount the drive again by clicking on the folder icon and then on the folder icon with the little Drive icon.\n",
        "\n",
        "Next, we need to import pandas and read the file!"
      ],
      "metadata": {
        "id": "rF9J9dgzci0k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = \"drive/MyDrive/jpt-full-no-0-authors.parquet\"\n",
        "\n",
        "df = pd.read_parquet(file_path)"
      ],
      "metadata": {
        "id": "7RByXUT5cgz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "PNpR7foEdOwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. We Turn our List of Articles into a List of Authors.\n",
        "\n",
        "`df` is a dataset of articles, and each article has a `creator` column, which has a list inside that is a list of all the authors. We want a list with all the authors only. What we will do is:\n",
        "\n",
        "1. Create an empty, master list of authors.\n",
        "1. Iterate over `df`.\n",
        "  1. For every article, we pull out the list of authors and then iterate over the list of authors\n",
        "    1. For every author, we add them to the master list of authors.\n",
        "1. The master list of authors will now have all the authors.\n",
        "1. We turn the master list into a master **Set**, which will remove duplicates for us.\n",
        "1. We now have a list of all the unique authors.\n",
        "\n",
        "This may sound like a lot, but it is rather straight forward in just a few lines of code."
      ],
      "metadata": {
        "id": "qJOpBrywd7Yx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "master_author_list = [] # empty list\n",
        "def add_creators_to_master_list(creator_list):\n",
        "  for creator in creator_list:\n",
        "    master_author_list.append(creator)\n",
        "\n",
        "df[\"creator\"].apply(add_creators_to_master_list)"
      ],
      "metadata": {
        "id": "dmGyeUz9ezAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is not the most elegant solution, but it does get us where we need to go. Let’s see how many authors were collected."
      ],
      "metadata": {
        "id": "DrsDnSTDfpM4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(master_author_list)"
      ],
      "metadata": {
        "id": "CTUJQwp6fJCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Who are the most common duplicates?"
      ],
      "metadata": {
        "id": "aNmff0NgfzIA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.Series(master_author_list).value_counts().head(15)"
      ],
      "metadata": {
        "id": "ZSz33s8MfkRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Does that sound right?\n",
        "\n",
        "Now let's get rid of the duplicates."
      ],
      "metadata": {
        "id": "FxBs9_tTf_ng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unique_authors = list(set(master_author_list))\n",
        "len(unique_authors)"
      ],
      "metadata": {
        "id": "X_ok8HZkf42y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ten thousand out of twenty thousand authors are unique, meaning on average an author has two articles in _JPE_. No big deal."
      ],
      "metadata": {
        "id": "BnD2I4HPgcr8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. We ascertain the gender of each author\n",
        "\n",
        "We will start out by using the [gender-guesser](https://pypi.org/project/gender-guesser/) library in Python. But we have to install it. Unlike pandas, it is not built into the Colab environment."
      ],
      "metadata": {
        "id": "2VVyvGpYl2Wh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install gender_guesser"
      ],
      "metadata": {
        "id": "WaOhAU4nmmZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gender_guesser.detector as gender\n",
        "detector = gender.Detector()"
      ],
      "metadata": {
        "id": "BZsrBF_wmbW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Let’s have a look at a random sample of 15 authors. Let's feed all 15 names into the gender detector."
      ],
      "metadata": {
        "id": "Cwdb3Hu4hQ3B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_of_15 = pd.Series(unique_authors).sample(15, random_state=15)\n",
        "sample_of_15"
      ],
      "metadata": {
        "id": "QSHTjovNgSaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name in sample_of_15:\n",
        "  print(f\"{name} is coded as {detector.get_gender(name)}\")"
      ],
      "metadata": {
        "id": "PSiscfBqnH0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's entirely useless. If we look at the manual for `gender-guesser`, we see:\n",
        "\n",
        "```python\n",
        ">>> print(d.get_gender(u\"Bob\"))\n",
        "male\n",
        ">>> print(d.get_gender(u\"Sally\"))\n",
        "female\n",
        ">>> print(d.get_gender(u\"Pauley\")) # should be androgynous\n",
        "andy\n",
        "```\n",
        "\n",
        "Oh. It only works on first names. Well, we can use the `.split()` method on a string to split a string into a list and then only test the first value of the list. As an example:"
      ],
      "metadata": {
        "id": "0FR1W223nTSz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Charles S. Ascher\".split(\" \"))\n",
        "print(\"Charles S. Ascher\".split(\" \")[0])"
      ],
      "metadata": {
        "id": "wQXxQ9bOnv7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s try again!"
      ],
      "metadata": {
        "id": "9edwayADn5TK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for name in sample_of_15:\n",
        "  first_name = name.split(\" \")[0]\n",
        "  print(f\"{name} is coded as {detector.get_gender(first_name)}\")"
      ],
      "metadata": {
        "id": "HxJVXtsXn825"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In short, we will not be able to ascertain the gender of someone based on their first initial even under the best of circumstances.\n",
        "Let’s create a new data frame with all these names, though, and see what we can do."
      ],
      "metadata": {
        "id": "JVFLuUatousN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "names_df = pd.DataFrame(unique_authors, columns=[\"full_name\"])\n",
        "names_df.sample(15, random_state=15)"
      ],
      "metadata": {
        "id": "814fgaOLpfC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create a new column with just the first name."
      ],
      "metadata": {
        "id": "PP95ybROpvD7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "names_df[\"first_name\"] = names_df[\"full_name\"].apply(lambda full_name: full_name.split(\" \")[0])\n",
        "names_df.sample(15, random_state=15)"
      ],
      "metadata": {
        "id": "v5wdMR9Hp6wy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OK. Not too shabby. Now let's add a column for gender."
      ],
      "metadata": {
        "id": "_IHN54lmq9nR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "names_df[\"gender\"] = names_df[\"first_name\"].apply(lambda first_name: detector.get_gender(first_name))\n",
        "names_df.sample(15, random_state=15)"
      ],
      "metadata": {
        "id": "tuQseqGurBxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice these two times with `.apply()` we did not define a function ahead of time. Instead, we used a [lambda function](https://realpython.com/python-lambda/). These are anonymous functions that we can use when we need to do a transformation quickly on the fly. So, above,\n",
        "\n",
        "```python\n",
        "names_df[\"first_name\"].apply(lambda first_name: detector.get_gender(first_name))\n",
        "```\n",
        "\n",
        "is the same as\n",
        "\n",
        "```python\n",
        "def get_gender(first_name):\n",
        "  return detector.get_gender(first_name)\n",
        "\n",
        "names_df[\"first_name\"].apply(get_gender)\n",
        "```\n",
        "\n",
        "From my understanding, lambdas are not particularly idiomatic Python, but I am very used to them from JavaScript, where they are ubiquitous (and are called anonymous functions).\n",
        "\n",
        "For longer functions that involve some kind of control flow, it makes sense to define a function. But for one-liners, a lambda is usually ok!\n",
        "\n",
        "Back to gender, though. Let's see the breakdown of genders."
      ],
      "metadata": {
        "id": "DEeubkcArR_G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "names_df[\"gender\"].value_counts()"
      ],
      "metadata": {
        "id": "d9oB3sUjst4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This probably is not telling us much that we don’t already know. _JPE_ is printing mostly men. And, in fact, even if all the edge cases were reclassified as women, it would still account for only 1/3 of the total number of authors. Let's look at the list of unknown gender people and see if it might make sense just to drop all of them.\n",
        "\n",
        "Notice, I'm going to filter the data frame based on gender and then look at the value counts for first names in one line of code."
      ],
      "metadata": {
        "id": "Zqe7VrKJtB_O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "names_df[names_df[\"gender\"] == \"unknown\"][\"first_name\"].value_counts()"
      ],
      "metadata": {
        "id": "pX2X0RW3tos8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So of the 2865 unknowns, there are 823 distinct first names. And we can see that some of the names look like legit first names, but the gender guesser opted not to guess."
      ],
      "metadata": {
        "id": "ERYC0TfLt5X1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. We Match our Name-to-Gender Lookup Table to the Original List of Articles so We Can See Which Articles Have Women Authors"
      ],
      "metadata": {
        "id": "_FVfSMb_u6uW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have a one of six (`male`, `female`, `mostly male`, `mostly female`, `unknown`, and `andy`) genders assigned to all of our unique authors, we can feed that information back into our original `df`. But we will have to be careful.\n",
        "\n",
        "First, let's just create six gender counter columns for `df` and set their values all to 0. Notice how simple it is to set an entire column with one value.\n"
      ],
      "metadata": {
        "id": "IqTAceeWhpMz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gender_counter_columns = [\"male_count\", \"female_count\", \"mostly_male_count\", \"mostly_female_count\",\n",
        "                            \"unknown_count\", \"andy_count\"]\n",
        "for gender_counter_column in gender_counter_columns:\n",
        "    df[gender_counter_column] = 0\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "bNZIAJlazOtJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now is when things get tricky:\n",
        "\n",
        "* Iterate over every row in `df` and with each row:\n",
        "  * Create a dictionary with all the gender counters and fill it\n",
        "    with the current counter values (will be 0).\n",
        "  * Iterate over the list of authors and for every author,\n",
        "    * Match the name with the name in `df_names` using the `.loc()` method\n",
        "    * Increment the matching gender counter in the dictionary by 1.\n",
        "  * Return the gender counters dictionary.\n",
        "\n",
        "Notice below how closely the actual function matches the shape of the bullet points above. This is very helpful when designing functions and algorithms."
      ],
      "metadata": {
        "id": "puWer0zf7WA2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_genders(row):\n",
        "  # First, we create a dictionary for all six gender counters.\n",
        "  gender_counters = {}\n",
        "  # We fill the counters with the current values from df. These will all be 0.\n",
        "  for gender_counter_column_name in gender_counter_columns:\n",
        "    gender_counters[gender_counter_column_name] = row[gender_counter_column_name]\n",
        "  # Next, we iterate over the list of authors.\n",
        "  for author in row[\"creator\"]:\n",
        "    # Match the name to the name in df_names and get the gender\n",
        "    # This could cause an error if no author exists with that name,\n",
        "    # But we have not dropped any articles, so every author should be\n",
        "    # findable.\n",
        "    #\n",
        "    # This is a complex line of code, but it basically says:\n",
        "    # Filter the names_df dataframe and give me a new dataframe where\n",
        "    # the \"full_name\" is equal to the author variable above.\n",
        "    # Next, give me the \"gender\" column of that dataframe, so now I\n",
        "    # have a series.\n",
        "    # Then, give me the first row in the series using .iloc[0].\n",
        "    detected_gender = names_df.loc[names_df[\"full_name\"] == author][\"gender\"].iloc[0]\n",
        "\n",
        "    # This `gender` variable is equal to one of the six genders, but we want\n",
        "    # to convert the string \"mostly male,\" say, into the counter column\n",
        "    # \"mostly_male_counter\".\n",
        "    gender_counter_column_name = detected_gender.replace(\" \", \"_\") + \"_count\"\n",
        "\n",
        "    # Now that we have the correct gender counter column name, we know what to\n",
        "    # increment in the dictionary.\n",
        "    # The `+=` increments the left-hand side by the right-hand side.\n",
        "    gender_counters[gender_counter_column_name] += 1\n",
        "\n",
        "  # And last, we return the dictionary with all of the gender counters\n",
        "  return gender_counters"
      ],
      "metadata": {
        "id": "zNg6uQ5QxX6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above function is really only doing a few small things, but they are cognitively tricky and there are nested loops: we are iterating over each article and then, for each article we are iterating over each author. But now we can run this as one line of an `.apply()` call and we'll be done!\n",
        "\n",
        "Below I do two things so I can update all six gender counter columns at once:\n",
        "1. I’m feeding the list of columns to the `[]` operator on the left-hand side\n",
        "1. By passing in the keyword argument `result_type=\"expand\"`, pandas is expecting a dictionary as the return value to the `.apply()` method (the return value of `count_genders` here), and then it uses the keys of the dictionary to know which columns in the dataframe to update.\n",
        "\n",
        "This is the first “big” command we are running, because it has to iterate over 10k articles and for each article it has to iterate over the 10k-long list of names at least once (if there is only one author), meaning it is doing $10,000^2$ operations, _minimum_. And still it only takes 30 seconds."
      ],
      "metadata": {
        "id": "7fvkvnTxDyEg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[gender_counter_columns] = df.apply(count_genders, axis = 1, result_type=\"expand\")"
      ],
      "metadata": {
        "id": "_D_aYqoMEFzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can sort the articles by number of authors and see the top 10 to make sure that it seems like the number of gender counts sums to the author count, and it looks like it does."
      ],
      "metadata": {
        "id": "tgii3d4je7U-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.sort_values(\"author_count\", ascending=False)[[\"author_count\", \"male_count\", \"female_count\", \"mostly_male_count\", \"mostly_female_count\", \"unknown_count\", \"andy_count\"]].head(10)"
      ],
      "metadata": {
        "id": "NXjwCpJMFWA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analyzing Genders of Authors"
      ],
      "metadata": {
        "id": "6oiZEWJ8gcpn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s look at the breakdown of authors by gender again from the `names_df` dataframe in terms of percentages"
      ],
      "metadata": {
        "id": "xzFwGMfehKR8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "names_df[\"gender\"].value_counts() / len(names_df)"
      ],
      "metadata": {
        "id": "Z8u8S8bJFXF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Identifiably male authors are 65% of the total, unknown authors make up over a quarter, and identifiably female authors make up just 6% of the authors in _JPE_. But let's track these things over time."
      ],
      "metadata": {
        "id": "pcdIbGOJhepX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "jpe_by_year = df.groupby(\"publicationYear\")\n",
        "jpe_by_year[\"female_count\"].sum().plot()"
      ],
      "metadata": {
        "id": "7YVj2y6hFY24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What does this chart show? What are potential problems with this chart based on the charts we looked at in the previous session?\n",
        "\n",
        "Let’s plot all the three big genders and the total number of authors."
      ],
      "metadata": {
        "id": "y60Rnt8jiSXz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(10, 8))\n",
        "fig.tight_layout()\n",
        "jpe_by_year[\"author_count\"].sum().plot(ax=axes[0,0], ylabel=\"total authors\")\n",
        "jpe_by_year[\"female_count\"].sum().plot(ax=axes[0,1], ylabel=\"female authors\")\n",
        "jpe_by_year[\"male_count\"].sum().plot(ax=axes[1,0], ylabel=\"male authors\")\n",
        "jpe_by_year[\"unknown_count\"].sum().plot(ax=axes[1,1], ylabel=\"unknown authors\")"
      ],
      "metadata": {
        "id": "SgadQRqmilcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is this telling us? How can we make this analysis more sensitive? Where do we go from here?"
      ],
      "metadata": {
        "id": "rNS_C9EOl7ie"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What Is Text Analysis?"
      ],
      "metadata": {
        "id": "r9gCQVBztyot"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "In simplified terms, “text analysis,” with “natural language processing” or “NLP,” can be understood as methods that allow computers to algorithmically “read” textual data. There are many techniques that we can use to break apart our data:\n",
        "\n",
        "* **Sentence parsing** We can break up the sentences in our corpus into subjects, verbs, and objects and analyze those items. Or we can see which prepositions are used most often and how. This practice relies on good “grammars” in place for your specific language, so it knows how to find a subject, verb, etc.\n",
        "* **Topic modeling** This technique analyzes _documents_ in a corpus and tries to find particular words that link specific documents together. Although the word “topic” suggests that there is a semantic meaning to these groupings (they are arranged “by topic,” or “by what they are about”), the algorithm does not actually understand, semantically, what words are related to other words.\n",
        "* **Classification** Similarly, we can use text analysis to classify documents based on certain features. This is a popular way to use machine learning with text analysis.\n",
        "* **Sentiment Analysis** We can also use NLP to do vibechecks on our documents. If you have seen articles about how negative Facebook posts attract more attention that positive ones, the authors are likely using sentiment analysis to classify the posts as positive or negative.\n",
        "* **Named Entity Recognition** NLP can find proper names and make reasonable guesses as to whether they refer to people, places, or institutions. This lets the researcher collect all the places in a corpus and build a map from that geographical data."
      ],
      "metadata": {
        "id": "2431j82trMks"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Some Terminology"
      ],
      "metadata": {
        "id": "V91F0EFIuAQB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Corpus** In NLP, a “corpus” is typically a collection of **documents**.\n",
        "* **Document** A “document” is a collection of text, typically unified in some way. A document can be any length; a tweet can be a document. The document provides the logical structure for classifying and organizing text objects. If you want to classify a handful of paragraphs from a novel as happy or sad, it probably makes sense to create a document for each paragraph and then analyze the paragraphs separately.\n",
        "* **Token** and **Tokenization** A “token,” in simplest terms, corresponds more or less to a word. “Tokenization” is the process of taking a document from a corpus and breaking it up into words. This is actually a more complex process than it may initially seem. What kinds of obstacles could make tokenization difficult for a computer?\n",
        "* **Lemma** and **Lemmatization** A “lemma” is the base root of a word, and “lemmatization” refers to the process whereby our NLP tools determine the root of a word. For example, “sings,” “singing” and “sang” all have the same lemma, “sing.”\n",
        "* **Stop Words** Often we are not interested in tracking the use of certain words, particularly very common ones like “a” and “the.” Stop words are a list of words that we tell our algorithims to ignore.\n",
        "* **Bag of Words** In many NLP applications, word order, paradoxically, does not matter. We say that these applications are working with a “Bag of Words.”\n"
      ],
      "metadata": {
        "id": "OzXtrxgRuCYd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Python NLP Libraries"
      ],
      "metadata": {
        "id": "iv4TEghp4USS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are many libraries in Python that help with text analysis and NLP. I will mention four here:\n",
        "\n",
        "1. [NLTK](https://www.nltk.org/) is the best known library for NLP in Python, and it more or less established the field. It has 50 modules or so for doing all kind of text-related analysis. The “[NLTK Book](https://www.nltk.org/book/)” is a deep introduction to the library. The library [TextBlob](https://textblob.readthedocs.io/en/dev/index.html) sits atop NLTK and may make it easier for some users to use.\n",
        "\n",
        "2. [spaCy](https://spacy.io/) is a new library. Unlike NLTK, spaCy limits options to let users get up and running with it quickly. It is also very, very fast and designed for production use (on websites, cell phones, etc.). [Quickstart](https://spacy.io/usage)\n",
        "\n",
        "2. [Textacy](https://textacy.readthedocs.io/en/latest/) is a wrapper for spaCy that helps with preparing documents for spaCy. [Quickstart](https://textacy.readthedocs.io/en/latest/quickstart.html)\n",
        "\n",
        "3. [Gensim](https://radimrehurek.com/gensim/#) is very purpose driven towards topic modelling and document organization and analysis. [Quickstart](https://radimrehurek.com/gensim/auto_examples/index.html#core-tutorials-new-users-start-here)\n",
        "\n",
        "4. As a robust data analysis library, [scikit-learn](https://scikit-learn.org/stable/modules/decomposition.html) also has modules that help with classification and topic modeling. They have a [specific tutorial on working with text data](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html#)."
      ],
      "metadata": {
        "id": "Hk_XZMP34cDx"
      }
    }
  ]
}