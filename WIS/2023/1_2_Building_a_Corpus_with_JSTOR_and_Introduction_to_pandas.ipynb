{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1KjdwUo72h_qGAZuF0AeG_WMo36BzRDVH",
      "authorship_tag": "ABX9TyM/wcM4E9jMMZvT07dvYWjS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/columbia-data-club/meetings/blob/main/WIS/2023/1_2_Building_a_Corpus_with_JSTOR_and_Introduction_to_pandas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![A blue background with the Python logo and the words Data Club on it](https://raw.githubusercontent.com/columbia-data-club/meetings/main/assets/images/data-club-python.png)\n",
        "\n",
        "## Building a Corpus with JSTOR and an introduction to pandas\n",
        "\n",
        "Dec. 1, 2023\n",
        "\n",
        "by [Moacir P. de Sá Pereira](https://moacir.com) for [Women in STEM @ SIPA](https://sipa.campusgroups.com/wis/home/), modified from  [Columbia Data Club](https://github.com/columbia-data-club/) notebooks by Roger Creel, Isha Shah, and others in the Data club past.\n",
        "\n",
        "This notebook underpins a ~75 minute presentation that uses a corpus of articles downloaded from [JSTOR](http://www.jstor.org) as an occasion to learn the basics of [pandas](https://pandas.pydata.org/) for complete beginners to Python and to programming."
      ],
      "metadata": {
        "id": "hi_tyqz_iV7W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data and Corpora"
      ],
      "metadata": {
        "id": "F47nNYCJgVuW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "When we want to analyze data, no matter what the data are, we have to acquire them somehow. Sometimes this takes the form of collecting or measuring our own data, such as with a GPS tracker or with surveys of people conducted in the field. Other times this involves acquiring data from other sources.\n",
        "\n",
        "If our data are textual in nature, it is very likely that we will be acquiring or compiling the data from some already existing repository, such as connecting to a website and downloading its textual data or interacting with a purpose-built dataset generator.\n",
        "\n",
        "Today, we will be working with such a generator, at least to see how it works. The actual data we will download from a link I provide in the notebook that will only work during this workshop. That means that after today, you will have to build up the data yourself if you wish to reproduce this notebook.\n",
        "\n",
        "Our collection of documents will make up a corpus (pl. corpora). We anticipate that every document in our corpus will have more or less the same shape; that is, the metadata will be predictable, and what works to get the names of the authors, for example, from one document will work for every document. This is a **luxury**. Often data collected from different sources will have different shapes and properties, and combining all the data into one predictable format can be very time-consuming. Today, we cheat a little to move more quickly."
      ],
      "metadata": {
        "id": "rXdPecLijW4o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Constellate and Other Corpus Building Tools"
      ],
      "metadata": {
        "id": "5o-Q2YGogejs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "[JSTOR](http://jstor.org), the non-profit repository of academic journals, launched a tool, [Constellate](http://www.constellate.org), a few years ago to facilitate entry-level macroanalysis of the contents of the JSTOR archive. The Libraries do not subscribe to these entry-level features, but we do maintain access to the downloads of datasets we build in Constellate. We will use Constellate today to download the contents of the _Journal of Political Economy_, or at least all of what JSTOR has, which as of today runs through 2017.\n",
        "\n",
        "Constellate, of course, is limited to the documents (articles, etc.) that JSTOR has. If you are eager to build a corpus using materials unavailable in JSTOR, please reach out to us at Research Data Services (`data@library.columbia.edu`), and we will try to help you. Alternatively, you can look at some of the resources I have collected in our [Text and Data Mining Guide](https://guides.library.columbia.edu/text-mining).\n",
        "\n",
        "One particular tool worth mentioning is ProQuest's [TDM Studio](https://guides.library.columbia.edu/text-mining/proquest-tdm-studio). This tool lets users build datasets out of newspaper articles, including _The New York Times_. Unlike Constellate, TDM Studio does not let you download the data to your own computer. Instead, ProQuest provides a virtual computer where you can use the data in a Jupyter notebook, much like what we are doing today with these Colab notebooks."
      ],
      "metadata": {
        "id": "SsgI53L0lPCh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Constellate\n"
      ],
      "metadata": {
        "id": "Y9oVNx4Zghc5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "![The front page of the Constellate website](https://raw.githubusercontent.com/columbia-data-club/meetings/main/WIS/assets/img/constellate/01-front-page.png)\n",
        "\n",
        "Navigating to [Constellate](http://constellate.org) will grab us with a lot of different things to explore at our leisure, but we will jump straight to logging in. Under the login, we select to log in with Google and use our Lionmail account to access Constellate. Once we log in, we are brought to the Dashboard, but we can immediately move to the dataset builder by clicking on “Build a Dataset.”\n",
        "\n",
        "![The Constellate Dataset Builder](https://raw.githubusercontent.com/columbia-data-club/meetings/main/WIS/assets/img/constellate/03-dataset-builder.png)\n",
        "\n",
        "\n",
        "The Dataset Builder is a bit finicky. We cannot just type in the publication title we want (or, at least, I am incapable of getting it to work). Instead we have to browse the titles, tick the box for _Journal of Political Economy_, and then scroll to the top and choose “Search Journals.” Now in the builder we will see the name _Journal of Political Economy_ underneath the Publication Titles filter, and we will see that Constellate is offering 13k documents instead of 33m. This is helpful, since Constellate maxes out at 25k documents per dataset, but, of course, once we download the datasets we can combine them. Once we confirm that this is the dataset we want, we click on “Build,” give the dataset a nickname, and it takes a few minutes to build the dataset. Once it is ready, it appears in our collection of datasets, and we can proceed to download.\n",
        "\n",
        "![The Constellate Dataset download options](https://raw.githubusercontent.com/columbia-data-club/meetings/main/WIS/assets/img/constellate/04-download-options.png)\n",
        "\n",
        "The first two download options provide sampled data from the entire dataset. We want everything, though. Under “More Download Options,” we can find the option to download everything. It will take several minutes for Constellate to prepare the download and then more time to do the actual download. To keep things moving, however, I have pre-setup the dataset and made it available for you all for the duration of this workshop, so lets download it!"
      ],
      "metadata": {
        "id": "AS-YJ6eLny9y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting our Data into Colab\n"
      ],
      "metadata": {
        "id": "3hZnNrB7gHJ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to download the data that I have made available on my Google Drive into the Colab environment. There are a few ways to do this, including ways that work entirely in Python, but we will do it a little bit old school:\n",
        "\n",
        "https://drive.google.com/file/d/19NSasyWmdcQS9oM372FdN-Qme_qw9XZQ/view?usp=drive_link\n",
        "\n",
        "Download this file (it is about 70Mb) to your computer, and click on the little folder to the left of this text in Colab. The file sidebar will open, and drop the file in there.\n",
        "\n",
        "**The Colab is temporary!** When this notebook is closed or has to restart, the file will be deleted, which is why Google warns you to have your ensure you have a copy on your own computer!"
      ],
      "metadata": {
        "id": "79z_dHpv1Ypo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A Word about File Formats"
      ],
      "metadata": {
        "id": "zVNcom-xf9vd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Constellate provides its data in the `jsonl` or \"line delimited JSON\" or \"JSON lines\" format. You will likely get your data in any of a number of formats, but there are a few guidelines to follow when saving your own:\n",
        "\n",
        "* Unless you are working with “big data” (gigabytes, like we actually are today), it probably makes sense to stick to a plain text file format. For tabular data, that is typically a `csv`, or [comma-separated values](https://en.wikipedia.org/wiki/Comma-separated_values) file. Tabular data (that is, with rows and columns) often comes in the [`json`](https://en.wikipedia.org/wiki/JSON) format, which is also a plain text format.\n",
        "* At large enough sizes, it becomes more efficient to use open binary file formats, like [`parquet`](https://en.wikipedia.org/wiki/Apache_Parquet). Binary file formats provide many efficiencies, but they cannot be read by any plain text parser.\n",
        "* Our format today, `jsonl`, is a variant of `json`, where each “row” is its own line. This is in contrast to typical `json`, where line breaks mean nothing.\n",
        "* If you have further questions, ask us!\n",
        "\n",
        "In a Python context, we can consider a `json` or `jsonl` file as a file that, when read by a Python interpreter, outputs a **list** made up of several **dictionaries**, where each dictionary, typically, has the same **keys**, though that is not a requirement. So, for example, a regular `json` file could look like:\n",
        "\n",
        "```python\n",
        "[\n",
        "  {\n",
        "    \"name\": \"Aisha\",\n",
        "    \"id\": 452\n",
        "  },\n",
        "  {\n",
        "    \"name\": \"Becky\",\n",
        "    \"id\": 792\n",
        "  },\n",
        "  {\n",
        "    \"name\": \"Chanda\",\n",
        "    \"id\": 223\n",
        "  }\n",
        "]\n",
        "```\n",
        "\n",
        "The same content as a `jsonl` file would look like:\n",
        "\n",
        "```python\n",
        "{\"name\": \"Aisha\", \"id\": 452}\n",
        "{\"name\": \"Becky\", \"id\": 792}\n",
        "{\"name\": \"Chanda\", \"id\": 223}\n",
        "```\n",
        "\n",
        "And as actual python code:"
      ],
      "metadata": {
        "id": "JvdKD1Gp4GGZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "team = [{\"name\": \"Aisha\", \"id\": 452}, {\"name\": \"Becky\", \"id\": 792}, {\"name\": \"Chanda\", \"id\": 223}]\n",
        "print([person[\"name\"] for person in team])"
      ],
      "metadata": {
        "id": "Bw5BvYWPx6DZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pandas"
      ],
      "metadata": {
        "id": "9jJHlXergM4O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s now move to [pandas](https://pandas.pydata.org/pandas-docs/stable/index.html). Pandas is a library for analyzing data, and it is often the first point of entry to any Python project involving data. It provides an idiosyncratic but predictable syntax for organizing your data the way that you want.\n",
        "\n",
        "The primary structure in pandas is the **DataFrame**. This is a two dimensional table that organizes your data into rows and columns. In other words, it is very similar to a _single_ Excel spreadsheet or Google Sheets sheet.\n",
        "\n",
        "It is customary in Python to import pandas as `pd` and to call your DataFrame `df`. You can do whatever you like, of course, but I recommend sticking to this tradition for today.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Q4KpGxfWyjnB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Getting Data into pandas\n",
        "\n",
        "But let’s read in our file we uploaded. Pandas can read many different formats, including Excel spreadsheets, `csv` files, regular `json` files, and more."
      ],
      "metadata": {
        "id": "B6GmI9iBJwwV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd # we will refer to pandas as \"pd\" going forward\n",
        "import matplotlib.pyplot as plt # we may do some plotting with matplotlib"
      ],
      "metadata": {
        "id": "Xqjxh0bHyOIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_json(\"jpe-full-nograms.jsonl\", lines=True)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "whZHT_Zl0pcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have a variable, `df`, that holds the contents of our file. Running the `.head()` method displays the first five rows (by default) in a slightly interactive format."
      ],
      "metadata": {
        "id": "0gz4U29Q1CE5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Initial Exploratory Data Analysis\n",
        "\n",
        "\n",
        "The information here is somewhat unwieldy, so let's look at the 27 columns that we have and consider dropping some."
      ],
      "metadata": {
        "id": "Izux3imfJ8M6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "BALFSv6G02um"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s also see what kinds of types are in these columns."
      ],
      "metadata": {
        "id": "If_wyglUjl94"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.dtypes"
      ],
      "metadata": {
        "id": "Th3Y4j4ZjpxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s take a few moments to consider all of these column names. Are there any we definitely want? Are there any we might want to drop? Are there any we want to know more about?\n",
        "\n",
        "We can use a few methods, `.describe()` and `.value_counts()` to get a sense of some of these columns."
      ],
      "metadata": {
        "id": "98guTKXM1oQq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['docType'].value_counts()"
      ],
      "metadata": {
        "id": "n3QvhV7C1lP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['publicationYear'].describe()"
      ],
      "metadata": {
        "id": "m9iDWO5f2I5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['wordCount'].describe()"
      ],
      "metadata": {
        "id": "45tByffG2UVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One thing I noticed when downloading this data was that the `fullText` column was often empty when I was sampling the dataset. I didn’t look into it systematically, but we are supposed to have the full text here, so one question is “how many articles actually have nothing in the `fullText` column?”"
      ],
      "metadata": {
        "id": "EhHpGa6M27ez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['fullText'].isnull().sum()"
      ],
      "metadata": {
        "id": "PCvuhRQK2ZP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"{round(100 * df['fullText'].isnull().sum()/len(df))}% of the entries have no fullText\")"
      ],
      "metadata": {
        "id": "8kD51kew24p1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What about authors, or “creators” in JSTOR’s language? Let’s check the publication year, too."
      ],
      "metadata": {
        "id": "LCv6uKc94TjN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for columnName in [\"fullText\", \"creator\", \"publicationYear\"]:\n",
        "  print(f\"{round(100 * df[columnName].isnull().sum()/len(df))}% of the entries have no {columnName}\")"
      ],
      "metadata": {
        "id": "rpY9Tw-d3bWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Excellent. We can handle a drop off on full text, but it’s good to see most of the articles have a creator and all have a publication year.\n",
        "\n",
        "Let’s look into the articles that don't have full text, though."
      ],
      "metadata": {
        "id": "Lm-iWtu94su8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[df[\"fullText\"].isnull()][\"publicationYear\"].hist()"
      ],
      "metadata": {
        "id": "CJHVrONX5oWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[df[\"fullText\"].notnull()][\"publicationYear\"].hist()"
      ],
      "metadata": {
        "id": "LdmKKpEY6Bw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It looks like it might be clear what articles are provided with full text and what articles aren’t!"
      ],
      "metadata": {
        "id": "UCS7IZ7c6Ndz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[df[\"fullText\"].isnull()][\"publicationYear\"].describe()"
      ],
      "metadata": {
        "id": "gepVQYT86VGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[df[\"fullText\"].notnull()][\"publicationYear\"].describe()"
      ],
      "metadata": {
        "id": "lfi1KROf6c8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Well this is irritating, but it also explains why this file is only 70Mb!\n",
        "\n",
        "I reached out to the Constellate team between preparing this notebook and this workshop, and they told me that Constellate will not have the full data on demand anytime soon. I misunderstood what it was for. For many text analysis operations, the unigrams that Constellate includes are sufficient, but for analysis where word order matters, you need to fill out a [Data for Research request](https://jstor.libwizard.com/f/dfr-request). These are handled by hand, so there is about a 48 hour turnaround."
      ],
      "metadata": {
        "id": "9eq20Gmn6kVi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The `[]` Operator\n",
        "\n",
        "We learned in the first module this morning that `[]` attached to the end of a variable can access aspects of that variable. For example, in a list, we can use it to get a specific index, like `a[0]` will return the “zeroth” (first) element of the list, and `a[-1]` will return the last.\n",
        "\n",
        "In a dictionary, we can use `d['key']` to access the value associated with a specific key.\n",
        "\n",
        "In pandas, the `[]` operator lets us do many, many things!\n",
        "\n",
        "* If we pass just a **string** (that is a column name), we get that entire column:"
      ],
      "metadata": {
        "id": "ZUljr_MTKJRF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "single_column_name_string = \"docType\"\n",
        "df[single_column_name_string]"
      ],
      "metadata": {
        "id": "h7yVTJ0WK3TI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* If we pass a **list** of strings (column names), we get a collection of columns."
      ],
      "metadata": {
        "id": "as9cP4s5Memo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_columns = [\"docType\", \"docSubType\"]\n",
        "df[list_of_columns]"
      ],
      "metadata": {
        "id": "61VGVoCRMoQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* If we pass a **Series** (a special data type in pandas) made up of `True`/`False` statements, we can use that to filter our data frame. This one is complicated. First, let’s create a series"
      ],
      "metadata": {
        "id": "8AvnDMVUNFEP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"doi\"] == \"10.1086/250019\""
      ],
      "metadata": {
        "id": "HpiUMCHNNFrR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(df[\"doi\"] == \"10.1086/250019\"))\n",
        "print(type(df))"
      ],
      "metadata": {
        "id": "T4V2WduGOrlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(df[\"doi\"] == \"10.1086/250019\").value_counts()"
      ],
      "metadata": {
        "id": "zd1wkBhbOgYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now if we feed that Series into the `[]` operator, it will work like a filter:"
      ],
      "metadata": {
        "id": "mrDr_DUiO5Qr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note the double `df`!\n",
        "df[df[\"doi\"] == \"10.1086/250019\"]"
      ],
      "metadata": {
        "id": "G36ZTACG4JUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the same as the previous cell\n",
        "doi_series = df[\"doi\"] == \"10.1086/250019\"\n",
        "df[doi_series]"
      ],
      "metadata": {
        "id": "BJZ3-D-IPebE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We already used this double `df` operation above when filtering out articles that had empty fields for `fullText` and so on:\n",
        "\n",
        "```python\n",
        "df[df[\"fullText\"].notnull()][\"publicationYear\"].describe()\n",
        "```\n",
        "\n",
        "In the first `[]` we filter based on rows where `fullText` is `notnull()`. The we add a second `[]` to limit our results to just the `publicationYear` column. Then we attach the standard pandas method `.describe()`, which works on dataframes and series, to get information about the series.\n",
        "\n",
        "This filtering takes getting used to, but it is pretty ergonomic and powerful."
      ],
      "metadata": {
        "id": "So0jZli1PSLX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Grouping\n",
        "\n",
        "Another complex topic is using pandas’s groups. This is very similar to using pivot tables in Excel, but I will show how to create something powerful with our dataset. First, let's look at a sample of the `creator` column. What does it look like?"
      ],
      "metadata": {
        "id": "BynClNEuQZr7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"creator\"].sample(10, random_state=42)"
      ],
      "metadata": {
        "id": "8LGWV-dS46le"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the `[]` notation, they kind of look like lists, so let’s see if they actually are lists:"
      ],
      "metadata": {
        "id": "eIU3l6pCRfOq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# .iloc[] is how you capture a specific row based on its index.\n",
        "creators = df.iloc[1626][\"creator\"]\n",
        "print(type(creators))\n",
        "print(creators)"
      ],
      "metadata": {
        "id": "JVWqdzwJQ0a0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Excellent! Now I'm curious to see if there's a relationship between number of authors over time, like maybe the number of authors grows. So we have this list of authors for every article, but we don't have anything in the dataframe that turns that into a specific number. Let’s change that.\n",
        "\n",
        "First, let’s write a function that takes a row from our dataframe and outputs the number of authors:"
      ],
      "metadata": {
        "id": "ej3j_hDuSEY9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_author_count(row):\n",
        "  number_of_authors = len(row[\"creator\"])\n",
        "  return number_of_authors"
      ],
      "metadata": {
        "id": "z-36Gh7bRt1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The idea behind this function is it is passed a row, much like how we created the row with `df.iloc[1626]` above. Then it takes the `creator` column from that row and uses the `len()` function to get its length.\n",
        "\n",
        "Let’s test it with the row we used above."
      ],
      "metadata": {
        "id": "6KIMKsEGTFs5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "homan_and_noble = df.iloc[1626]\n",
        "number_of_authors = get_author_count(homan_and_noble)\n",
        "print(number_of_authors)\n",
        "print(homan_and_noble['creator'])"
      ],
      "metadata": {
        "id": "0ZAw79_kTaGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is where things get exciting. Let’s create a new column, called `author_count` for our dataframe and have it hold the value of this `author_count()` function we created. We’ll use the `apply` method, which iteratively executes a function on every row (if `axis = 1`) or column (otherwise) in the dataframe."
      ],
      "metadata": {
        "id": "pbDHB00eTzfo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"author_count\"] = df.apply(get_author_count, axis=1)"
      ],
      "metadata": {
        "id": "tOZRQSk0ToNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**An ERROR!** What did we do wrong? The message reads:\n",
        "\n",
        "> TypeError: object of type 'NoneType' has no len()\n",
        "\n",
        "And that, actually, makes sense. What it’s saying is that when the `creator` column is empty, we do not have an empty list, where `len([])` is 0. Instead, we have the special value `None`, which is Python’s version of `null`/`nil`. And if we try to take the length of `None`:"
      ],
      "metadata": {
        "id": "Hi5nyEMyUawF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(None)"
      ],
      "metadata": {
        "id": "GXIF8_S0UGZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We get the exact same error as above. This means we have to change our `author_count()` function to react one way when `creator` is a list, and another way when it is `None`. There are many ways to do this, but this is a quick one:"
      ],
      "metadata": {
        "id": "v_T0BlBbVGYF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_author_count(row):\n",
        "  if row[\"creator\"] == None:\n",
        "    return 0\n",
        "\n",
        "  number_of_authors = len(row[\"creator\"])\n",
        "  return number_of_authors"
      ],
      "metadata": {
        "id": "5ZWID1ENVFQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now…"
      ],
      "metadata": {
        "id": "_sA65agZVkdj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"author_count\"] = df.apply(get_author_count, axis=1)"
      ],
      "metadata": {
        "id": "qjBBshtaVjLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s investigate our new column!"
      ],
      "metadata": {
        "id": "_EkPuhl1VpSY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"author_count\"].describe()"
      ],
      "metadata": {
        "id": "Wwf9o8IAVoAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given that 75% of the articles have 2 or fewer authors, this may not be particularly illuminating, but let’s see what happens when we group things.\n",
        "\n"
      ],
      "metadata": {
        "id": "EquOvsWYWbUN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pub_year_group = df.groupby(\"publicationYear\")\n",
        "print(type(pub_year_group))\n",
        "pub_year_group"
      ],
      "metadata": {
        "id": "AKjrGFk9Vt-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, the group is not, by itself, interesting. Groups in pandas are sort of interstitial objects. You trigger them with the `.groupby()` method, but then you need to use a second method to get information out of them.\n",
        "\n",
        "Below, we extract the `author_count` column out of the group and use the `.max()` method to get the number of authors in the article with the most authors for every year."
      ],
      "metadata": {
        "id": "EKUHf-EKW5JE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pub_year_group[\"author_count\"].max()"
      ],
      "metadata": {
        "id": "Em57C77xWzgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pub_year_group[\"author_count\"].mean()"
      ],
      "metadata": {
        "id": "KYl_HEGEXOxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pub_year_group[\"author_count\"].mean().plot()"
      ],
      "metadata": {
        "id": "Nm-UITzrXu_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Does this tell us anything? Let's plot the number of articles published a year, too. I use the `id` column just because I assume every row has an `id`, and if I just plot the group by itself, it will plot all of the columns!"
      ],
      "metadata": {
        "id": "Mk1VP1xJYbfT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pub_year_group[\"id\"].count().plot()"
      ],
      "metadata": {
        "id": "At9Wjt_iX1H2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This isn’t my field, but already I have a handful of possible research topics to pursue!"
      ],
      "metadata": {
        "id": "P57e0sLVY6RM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Removing Data\n",
        "\n",
        "Pandas is non-destructive. When you make changes to your dataframe, it always returns those changes as a new dataframe (or series). That is why you will often see commands that look like:\n",
        "\n",
        "```python\n",
        "df = df...\n",
        "```\n",
        "\n",
        "This is a way of overwriting the data. But with notebooks, we can always go back to the top of the notebook and press play all over again and rebuild the data if we have made mistakes.\n",
        "\n",
        "Up above, I showed that using the `[]` operator with a list of column names will show you only those columns. That, it turns out, is an easy way to delete columns you don’t want by omission. But for this part of the exercise, we are going to remove all the rows (articles) that don’t have authors and see how the numbers for author counts change once we have at least one author.\n",
        "\n",
        "We do this by creating a series of `True`/`False` based on the `author_count`, and then we save the filtered dataframe _as_ `df`, overwriting the first df:"
      ],
      "metadata": {
        "id": "c2oin9tmbElY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"df is currently {len(df)} rows large\")\n",
        "df = df[df[\"author_count\"] > 0]\n",
        "print(f\"df is currently {len(df)} rows large\")"
      ],
      "metadata": {
        "id": "jRu60CocYT53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We lost around 1500 rows this way, but let's see if the plots change."
      ],
      "metadata": {
        "id": "uQhWppstcfzD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pub_year_group = df.groupby(\"publicationYear\")\n",
        "pub_year_group[\"author_count\"].mean().plot()"
      ],
      "metadata": {
        "id": "gYbiKkBIcd5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What does this new chart tell us?"
      ],
      "metadata": {
        "id": "DWJ02TwwdBvH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plotting with Matplotlib\n",
        "\n",
        "[Matplotlib](https://matplotlib.org/stable/) is the standard plotting library in Python. Pandas is already leveraging it in the plots above, but in order to exercise better control over the plot, we should acess Matplotlib directly.\n",
        "\n",
        "We imported it at the same time that we imported pandas, and we renamed it to `plt`. This is, again, a Python convention."
      ],
      "metadata": {
        "id": "bsSHTb4mdIaA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(x=df[\"wordCount\"], y=df[\"author_count\"])\n",
        "plt.title('Do many authors make articles longer?')\n",
        "plt.xlabel('Word Count')\n",
        "plt.ylabel('Author Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HJ-Ir3ldc1-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What seems to be the answer to the question posed by the title in this chart?"
      ],
      "metadata": {
        "id": "x3bFMNQeeZqB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exporting Data from pandas\n",
        "\n",
        "As a final step, we need to export our data so we can use it in the next module. Pandas can export to basically any format you might need, but we will use `parquet`.\n",
        "\n",
        "Furthermore, we’ll export to our Google Drives. First we have to mount the drives by clicking on the little folder icon to the left and then the folder with the triangular Drive logo. This will make a `drive` folder appear after a minute or so. It has a folder called `MyDrive`, and we'll save our file in there."
      ],
      "metadata": {
        "id": "AtyZVJabep9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_parquet(\"drive/MyDrive/jpt-full-no-0-authors.parquet\")"
      ],
      "metadata": {
        "id": "yPR4Q4QydxJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We may need to hit the little folder refresh icon on the side to see our new file, but let's get ready for part 3!"
      ],
      "metadata": {
        "id": "412nYW8CfRlq"
      }
    }
  ]
}