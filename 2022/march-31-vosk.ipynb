{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transcribing Audio with Vosk",
      "provenance": [],
      "collapsed_sections": [
        "ELUP3napFAMA",
        "JhQaO3gsQE18"
      ],
      "authorship_tag": "ABX9TyNXABTq2RSk/DICOgN2tw5L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cul-data-club/meetings/blob/main/2022/march-31-vosk.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transcribing Audio with Vosk (Offline!)\n",
        "\n",
        "by Moacir P. de SÃ¡ Pereira, [Research Data Services](https://library.columbia.edu/services/research-data-services.html), Columbia University Libraries\n",
        "\n",
        "A [2022 article in _Politico_](https://www.politico.com/news/2022/02/16/my-journey-down-the-rabbit-hole-of-every-journalists-favorite-app-00009216#:~:text=The%20Otter%20privacy%20policy%20claims,for%20sharing%20with%20third%20parties.) has raised (anew) issues surrounding speech recognition tools used for audio transcription, in this case the popular service [Otter](http://otter.ai), which has even been recommended for use at Columbia. ([Susan McGregor](https://datascience.columbia.edu/people/susan-mcgregor/), of Columbia's Data Science Institute, is quoted in the _Politico_ piece.)\n",
        "\n",
        "The issues are twofold: the first is giving third parties access to your data (in this case both the audio recordings themselves and your transcripts). This calls to mind the January 2022 scandal regarding the sharing of AI-chat transcripts for The Crisis Text Line ([also reported in _Politico_](https://www.politico.com/news/2022/01/28/suicide-hotline-silicon-valley-privacy-debates-00002617)) with third-party vendors.\n",
        "\n",
        "The second issue is that AI- (or machine learning-) driven speech recognition solutions are only as good as the training that the AI has received (or how good the training model is). We see the same phenomenon in real life, where if we take a car in for service, a mechanic with 20 years of experience will be able to determine the problem much more quickly than a teenager who has never popped the hood of a car. As a result, services like Otter, but also AI assistants like Siri and Alexa, eat their own dogfood: they take requests they have transcribed and add them to their model so that they can \"understand\" requests better in the future. This is, of course, also a concern for privacy advocates.\n",
        "\n",
        "## ML-Driven Speech Recognition with User Control\n",
        "\n",
        "The alternatives to the above systems (which include [Google's Cloud Speech-to-Text API](https://cloud.google.com/speech-to-text) are ones where the model used for transcription is in the hands of the transcriber at all times. On the one hand, transcribers can train their own models, which can be powerful but also time-consuming. For some languages or registers, a locally-trained model may be the only option. On the other hand, transcribers can make use of pre-trained models. These models don't change as they are fed new information, which means the privacy of the transcriber's source material is maintained.\n",
        "\n",
        "As with all ML-driven work, the model is a black-box whose only important parameter is efficacy. As a result, a handful of open source APIs have emerged for audio transcription, allowing researchers to investigate how the APIs function and further allowing researchers to use them offline, with no worry of data compromises.\n",
        "\n",
        "Here are a few open source speech recognition options:\n",
        "\n",
        "* Mozilla's [DeepSpeech](https://github.com/mozilla/DeepSpeech). As of 2020, [the status of this system is unclear](https://discourse.mozilla.org/t/future-of-deepspeech-stt-after-recent-changes-at-mozilla/66191), and it has been over a year since a release was cut.\n",
        "* [Kaldi](https://kaldi-asr.org/index.html), which includes [13 pre-trained models](https://kaldi-asr.org/models.html)\n",
        "* Facebook's Flashlight ML library includes [an automatic speech recognition app](https://github.com/flashlight/flashlight/tree/main/flashlight/app/asr) based on their earlier work, Wav2Letter++.\n",
        "* [Athena](https://github.com/athena-team/athena) is a Python-based speech processing engine built atop Google's popular [TensorFlow](https://www.tensorflow.org/) ML library."
      ],
      "metadata": {
        "id": "Vn4ZUv8UvQRq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enter Vosk\n",
        "\n",
        "For this notebook, however, we'll be focusing on relative newcomer [Vosk](https://alphacephei.com/vosk/). What attracts me to Vosk is that they ship models for twenty different languages, and that the models have small versions (50MB) designed for \"offline\" use on either embedded systems (Raspberry Pis) or smartphones. With such a light footprint, implementing Vosk in a Colab becomes rather trivial.\n",
        "\n",
        "Additionally, Vosk supports streaming recognition, which can provide real-time transcription, much like how Google's Speech-to-Text API powers the real-time subtitling of the [Studio Remote live stream](https://www.youtube.com/channel/UCLOUh6s8E2FYAVAsJg3lgoA). Finally, Vosk has wrappers for many languages other than just Python.\n",
        "\n",
        "With the rest of this notebook, we'll try three things:\n",
        "\n",
        "1. Run the Vosk tutorial to get a feel for the library.\n",
        "2. Implement an updated version [of this Colab](https://colab.research.google.com/gist/dauuricus/4d9ba614afd7558a2591451fe08949ef/vosk_chinese.ipynb#scrollTo=xNNdQKf4oU2Q) to learn how to pull a video from YouTube and process its speech.\n",
        "3. Upload bespoke audio (the beginning of this workshop) and process it."
      ],
      "metadata": {
        "id": "CI-JuOLl4lCb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Vosk tutorial\n",
        "\n",
        "This section of the notebook is based on the [Vosk-API Python example](https://github.com/alphacep/vosk-api/tree/master/python/example)."
      ],
      "metadata": {
        "id": "ELUP3napFAMA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Vosk and clone the Vosk-API to the Colab storage area\n",
        "\n",
        "import sys\n",
        "!{sys.executable} -m pip install vosk\n",
        "!git clone https://github.com/alphacep/vosk-api"
      ],
      "metadata": {
        "id": "357qGFbCFYBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Move some files from the Python example directory to the root directory.\n",
        "\n",
        "%mv vosk-api/python/example/test_simple.py ./test_simple.py\n",
        "%mv vosk-api/python/example/test.wav ./test.wav\n",
        "\n",
        "# Download, unzip, and rename the English model.\n",
        "# The built-in test script expects to find a model in a folder called \n",
        "# \"model\" in the same directory as the script.\n",
        "\n",
        "!wget https://alphacephei.com/kaldi/models/vosk-model-small-en-us-0.15.zip\n",
        "!unzip vosk-model-small-en-us-0.15.zip\n",
        "%mv vosk-model-small-en-us-0.15 model"
      ],
      "metadata": {
        "id": "ffn5UYMPGDeT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's play the audio file\n",
        "\n",
        "from IPython.display import Audio\n",
        "\n",
        "Audio(\"test.wav\")"
      ],
      "metadata": {
        "id": "-au50QHiHQUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's execute the sample script\n",
        "\n",
        "!{sys.executable} ./test_simple.py test.wav"
      ],
      "metadata": {
        "id": "zTYIXwoiHhFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our ears hear:\n",
        "\n",
        "> one zero zero zero one nine oh two one oh zero one eight zero three\n",
        "\n",
        "However, the model transcribes: \n",
        "\n",
        "> one zero zero zero one nah no to i know zero one eight zero three\n",
        "\n",
        "In other words, it's not perfect. But maybe it's a bit of a time saver. Note that the second phrase is idiomatic. Presumably, if the speaker said \"nine zero two one zero,\" the model would have understood, but using the idiomatic \"oh\" instead of \"zero\" may have suggested to the model that it's not listening to numbers.\n"
      ],
      "metadata": {
        "id": "KAG-YptkN_6V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vosk in Mandarin\n",
        "\n",
        "Now we can download a video from YouTube, strip the audio, and process it using the Chinese model. We'll use this six-minute video:"
      ],
      "metadata": {
        "id": "JhQaO3gsQE18"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import YouTubeVideo\n",
        "\n",
        "video_id = \"cNSq5RdVf28\"\n",
        "\n",
        "YouTubeVideo(video_id)"
      ],
      "metadata": {
        "id": "euvj9iFJMq9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets download the video and extract the audio\n",
        "\n",
        "video_url = f\"https://youtu.be/{video_id}\"\n",
        "\n",
        "!{sys.executable} -m pip install -q youtube-dl\n",
        "!youtube-dl --extract-audio --audio-format wav --output \"extract.%(ext)s\" {video_url}"
      ],
      "metadata": {
        "id": "SFxZ17qjQv_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The audio is in stereo, which will cause problems down the line. Let's\n",
        "# convert it to a PCM WAV here.\n",
        "!apt install ffmpeg\n",
        "!ffmpeg -i extract.wav -vn -acodec pcm_s16le -ac 1 -ar 16000 -f wav extract-mono.wav"
      ],
      "metadata": {
        "id": "4WUFV_Henvy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We now have a .wav file. What do we know about it?\n",
        "import librosa\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa.display\n",
        "\n",
        "def getWavInfo(file):\n",
        "  data, framerate = librosa.load(file)\n",
        "  print(f\"The framerate is {framerate} Hz\")\n",
        "  print(f\"The duration is {len(data)/framerate} s\")\n",
        "  plt.figure(figsize=(14, 5))\n",
        "  librosa.display.waveplot(data, sr=framerate)\n",
        "\n",
        "getWavInfo(\"extract-mono.wav\")"
      ],
      "metadata": {
        "id": "u2LCGwXsR35i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# And the test file, just for laughs?\n",
        "\n",
        "getWavInfo(\"test.wav\")"
      ],
      "metadata": {
        "id": "iuGBWa2sSawN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Next, let's get the Chinese model.\n",
        "\n",
        "!wget https://alphacephei.com/vosk/models/vosk-model-small-cn-0.3.zip \n",
        "!unzip vosk-model-small-cn-0.3.zip\n",
        "%mv vosk-model-small-cn-0.3 model-cn\n",
        "!rm -rf vosk-model*"
      ],
      "metadata": {
        "id": "2dpfrTsFVfiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# As noted above, the default test script expects a \"model\" folder, but now we\n",
        "# have a model folder for English, but a model-cn folder for Chinese. Also, I\n",
        "# think we can do better for output options, so let's re-tool the script:\n",
        "\n",
        "from vosk import Model, KaldiRecognizer, SetLogLevel\n",
        "import os\n",
        "import wave\n",
        "import json\n",
        "\n",
        "def transcribe_simple(file, model_path=\"model\"):\n",
        "  SetLogLevel(0)\n",
        "  if not os.path.exists(model_path):\n",
        "    print (\"Please download the model from https://alphacephei.com/vosk/models and unpack as 'model' in the current folder.\")\n",
        "    exit (1)\n",
        "  wf = wave.open(file, \"rb\")\n",
        "  if wf.getnchannels() != 1 or wf.getsampwidth() != 2 or wf.getcomptype() != \"NONE\":\n",
        "      print (\"Audio file must be WAV format mono PCM.\")\n",
        "      if wf.getnchannels() == 2:\n",
        "        print(\"Converting to mono\")\n",
        "      exit (1)\n",
        "  model = Model(model_path)\n",
        "  rec = KaldiRecognizer(model, wf.getframerate())\n",
        "  rec.SetWords(True)\n",
        "  while True:\n",
        "      data = wf.readframes(4000)\n",
        "      if len(data) == 0:\n",
        "          break\n",
        "      if rec.AcceptWaveform(data):\n",
        "          continue\n",
        "      else:\n",
        "          continue\n",
        "  return json.loads(rec.FinalResult())"
      ],
      "metadata": {
        "id": "FCTNC_QuWccu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# With the modified script, let's try out the test.wav.\n",
        "transcription = transcribe_simple(\"test.wav\")\n",
        "transcription['text']"
      ],
      "metadata": {
        "id": "qzxLjGc1byI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# And the Chinese wav?\n",
        "transcription = transcribe_simple(\"extract-mono.wav\", \"model-cn\")\n"
      ],
      "metadata": {
        "id": "R2i3KI96b1b-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transcription['text']"
      ],
      "metadata": {
        "id": "p3vbfQWGoXtx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vosk with Whatever\n",
        "\n",
        "Now we can move to our own files if we want to use them. Similarly, we can download [various other Vosk models](https://alphacephei.com/vosk/models) and extract audio from other YouTube videos. For example:\n",
        "\n",
        "- [Indian English (small)](https://alphacephei.com/vosk/models/vosk-model-small-en-in-0.4.zip)\n",
        "- [Russian (small)](https://alphacephei.com/vosk/models/vosk-model-small-ru-0.22.zip)\n",
        "- [French (small)](https://alphacephei.com/vosk/models/vosk-model-small-fr-0.22.zip)\n",
        "- [German (small)](https://alphacephei.com/vosk/models/vosk-model-small-de-0.15.zip)\n",
        "- [Spanish (small)](https://alphacephei.com/vosk/models/vosk-model-small-es-0.22.zip)\n",
        "- [Brazilian Portuguese (small)](https://alphacephei.com/vosk/models/vosk-model-small-pt-0.3.zip)\n",
        "- [Turkish (small)](https://alphacephei.com/vosk/models/vosk-model-small-tr-0.3.zip)\n",
        "- [Hindi (small)](https://alphacephei.com/vosk/models/vosk-model-small-hi-0.22.zip)\n",
        "- [Esperanto (small)](https://alphacephei.com/vosk/models/vosk-model-small-eo-0.22.zip)\n",
        "\n",
        "and so on... What will we transcribe next?"
      ],
      "metadata": {
        "id": "Vq413PJLrnO2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "eubkTSPBtSko"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}