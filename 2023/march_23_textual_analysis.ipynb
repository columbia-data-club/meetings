{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/columbia-data-club/meetings/blob/main/2023/march_23_textual_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![A blue background with the SQLite logo and the words Data Club on it](https://raw.githubusercontent.com/columbia-data-club/meetings/main/assets/images/data-club-textacy.png)\n",
        "\n",
        "# Exploratory Analysis of Textual Data\n",
        "\n",
        "March 23, 2023\n",
        "\n",
        "by [Moacir P. de Sá Pereira](https://moacir.com) for the [Columbia Data Club](https://github.com/columbia-data-club/).\n",
        "\n",
        "Working with unstructured, textual data in Python presents new challenges. We can use some of our familiar pandas idioms to organize our corpus of text documents, but even a surface knowledge of the corpus demands new tools for analyzing data. Here, we’ll build a corpus of text and begin looking for macro trends in it.\n",
        "\n",
        "This notebook was inspired by “[A Full Guide on Scraping Yahoo Finance](https://www.octoparse.com/blog/how-to-scrape-yahoo-finance),” by Octoparse."
      ],
      "metadata": {
        "id": "ABH8foDOT6D6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting the Text\n",
        "\n",
        "A corpus of text can come ready-made or it has to be collected by the researcher through a process that often calls for a certain amount of creativity. The Libraries’ [Guide on Text Mining](https://guides.library.columbia.edu/text-mining) includes a few resources for finding both free and licensed corpora. But often a ready-made corpus fits the needs of a researcher looking to improve an algorithm, not a researcher looking to glean information from the content of the corpus.\n",
        "\n",
        "In this latter case, the researcher has to build the corpus. If the text is online, there are usually three ways to do this:\n",
        "\n",
        "1. **Access the Site’s Database**: This is probably the easiest solution, but it’s also the least likely. Still, it probably does not hurt to reach out to the admin of a site you want to mine and ask for either a dump of their content or read access to their database. But, again, this is unlikely.\n",
        "1. **Access via the Site’s API**: If the site has a [REST API](https://restfulapi.net/) or similar, then it’s often possible to interact with the site programmatically to get the site’s content. For example, historically both [Twitter](https://developer.twitter.com/en/docs/twitter-api) and [Reddit](https://www.reddit.com/dev/api/) have had friendly APIs in the past that have since been somewhat curtailed. Twitter’s API is still rather open for academic researchers, but, of course, in March 2023, it’s hard to tell what’s exactly going on on Twitter. With Reddit, historical access to the content of posts seems no longer possible, but there are external tools like [PushShift](https://pushshift.io/) that can help.\n",
        "1. **Scrape the Site**: This is the technique we will be using. This is the **WORST OPTION** for two reasons: it’s the finickiest, as what works today might not work in the future with no prior warning, and, second, websites often **EXPLICITLY FORBID** scraping. This forbidding is often listed on the site’s `robots.txt` file, which tells webcrawlers if they are allowed to crawl the site and which parts are crawlable. However, sometimes, like in [Facebook’s `robots.txt` file](https://facebook.com/robots.txt), the limitations are spelled out in clear language.\n",
        "\n",
        "Since we’ll be scraping, let’s move on to strategies for doing that."
      ],
      "metadata": {
        "id": "E9b_N6jKVszv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scraping and Parsing the Internet\n",
        "\n",
        "First, sometimes [`wget`](https://www.gnu.org/software/wget/) is sufficient for collecting the contents of a website. A command-line tool, `wget` can recursively work its way through an entire sitemap, following links and downloading pages. \n",
        "\n",
        "Additionally, we could write our own webcrawling spider in Python using [Scrapy](https://scrapy.org/), which both downloads and parses html files, while `wget` only downloads.\n",
        "\n",
        "Instead, we’ll be doing our downloading the old fashioned way, with vanilla `HTTP` requests. We’ll do this using the [Requests](https://requests.readthedocs.io/en/latest/) library. As for parsing, we’ll use the [Beautiful Soup](https://beautiful-soup-4.readthedocs.io/en/latest/) library.\n",
        "\n",
        "That said, here’s where we need to start optimizing for the nature of the documents we hope to download."
      ],
      "metadata": {
        "id": "FIAb9f0rNUFe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Yahoo Finance at a Glance\n",
        "\n",
        "We’re going to be working with the [Yahoo Finance Latest News](http://finance.yahoo.com/news), which at the time of this typing, looks like this:\n",
        "\n",
        "![A screenshot of the Yahoo Finance news website](https://raw.githubusercontent.com/columbia-data-club/meetings/main/assets/images/yahoo-1.png)\n",
        "\n",
        "Visually, the page has a simple structure. There’s a header bar with navigation links and little widgets giving information about the state of the market. There’s a sidebar with a few articles on it and a footer just out of view, but the bulk of the news content is on an infinite scroll bar that loads articles as you keep scrolling. In a production environment, this would cause problems, but we’re keeping things light for this workshop.\n",
        "\n",
        "To smooth things along, we can investigate the semantic structure of the news page to see how it signals articles:\n",
        "\n",
        "![A view of the DOM of the Yahoo Finance news page](https://raw.githubusercontent.com/columbia-data-club/meetings/main/assets/images/yahoo-2.png)\n",
        "\n",
        "Note `<div id=\"Fin-Stream\"…>`. This is the container that holds the stream of articles, which are members of an unordered list. Each list member seems to have the shape of `<li class=\"js-stream-content Pos(r)\">`, so we can use that to parse out objects."
      ],
      "metadata": {
        "id": "V7CzIjl4PxOX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "def getWebPage(url):\n",
        "  try:\n",
        "    r = requests.get(url)\n",
        "    if r.status_code == 200:\n",
        "      return r.text\n",
        "    else:\n",
        "      raise Exception(f\"Status code for {url} was {r.status_code}\")\n",
        "  except:\n",
        "    print(f\"getWebPage failed on {url} ({r.status_code})\")"
      ],
      "metadata": {
        "id": "W6FYct_8V4I8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "webpage = getWebPage(\"https://finance.yahoo.com/news\")\n",
        "parsed_doc = BeautifulSoup(webpage, \"html.parser\")\n",
        "\n",
        "for i, article in enumerate(parsed_doc.find_all(\"li\", class_=\"js-stream-content\")):\n",
        "  # The article headline is in an <h3> tag\n",
        "  print(f\"{i + 1}. “{article.h3.get_text()}”\")"
      ],
      "metadata": {
        "id": "wGLJjgfGZaJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we naively grab the news page, then, we only get the top 26 articles. If we were a business that had a crawler ping the site every five minutes and grab the top 26, then this might be a useful way to construct a corpus. But if we want to do historical research on Yahoo Finance or get the news more systematically, this is not a particularly efficient way to go about doing this.\n",
        "\n",
        "Again, creativity."
      ],
      "metadata": {
        "id": "AtUC5MPIbCQg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scraping Yahoo Finance\n",
        "\n",
        "There are a few options even within Yahoo Finance. One is a robot working every five minutes. Going forward, that’s not a terrible idea. Going backward, however, it is. Luckily, Yahoo Finance publishes a systematic index of [all of its articles](https://finance.yahoo.com/sitemap/) on its sitemap. What’s more, the map is broken apart into days, meaning we can target a specific date or range of dates and download all the articles. \n",
        "\n",
        "The structure of the page is pretty straightforward: there is a `<ul>` inside `<div class=\"sitemapcontent\"…>` that holds the list of articles, where every article is `<li><a>Headline</a></li>`, more or less. As such, the code is not much different than above to see the list of articles.\n",
        "\n",
        "The Silicon Valley Bank failure, recently in the news, happened on March 10, 2023. Initially, I planned on getting every article published on Yahoo Finance on that day, but it’s over 1000 articles, so instead let’s just get about 500 and call it a day."
      ],
      "metadata": {
        "id": "yIM-PXbeXBUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "\n",
        "# Doing this in code should we want to automate getting more than\n",
        "# one day's worth of articles.\n",
        "date = datetime.date(2023, 3, 10)\n",
        "base_url = \"https://finance.yahoo.com/sitemap/\"\n",
        "\n",
        "articles_url = base_url + date.strftime(\"%Y_%m_%d\")\n",
        "webpage = getWebPage(base_url + date.strftime(\"%Y_%m_%d\"))\n",
        "parsed_doc = BeautifulSoup(webpage, \"html.parser\")\n",
        "sitemap_content = parsed_doc.find(\"div\", class_=\"sitemapcontent\")\n",
        "\n",
        "# print(sitemap_content)\n",
        "for i, article in enumerate(sitemap_content.find_all(\"li\")):\n",
        "  print(f\"{i + 1}. “{article.get_text()}”\")"
      ],
      "metadata": {
        "id": "KLTSS6PNSzUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we get 50 articles, but we want more, still. At the bottom of the list of articles is a \"Next\" button (and sometimes “Start” button) that takes the form of:\n",
        "\n",
        "```html\n",
        "<div>\n",
        "  <a \n",
        "    href=\"https://finance.yahoo.com/sitemap/2023_03_10_start{some epoch time}\"…>\n",
        "    Next\n",
        "  </a>\n",
        "</div>\n",
        "```\n",
        "\n",
        "We can look for this link and follow it when it’s available. However, now we’re about to start looping with our requests, which means it’s a good idea to space out the process a bit using `time.sleep()`. Many websites have limits on how many times you can hit their servers in a given period, and other sites may be set up to intercept and block scraping. In short, it’s polite not to slam a website with a flurry of requests."
      ],
      "metadata": {
        "id": "zJp5PHNHrtaJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def add_articles(url, page=1, articles=[]):\n",
        "  webpage = getWebPage(url)\n",
        "  parsed_doc = BeautifulSoup(webpage, \"html.parser\").find(\"div\", class_=\"sitemapcontent\")\n",
        "  for article in parsed_doc.find_all(\"li\"):\n",
        "    articles.append(article)\n",
        "\n",
        "  # We assume there is a sibling to the <ul> that holds the Next button\n",
        "  nav_buttons = parsed_doc.ul.next_sibling\n",
        "  # Stop when we hit 10 pages (500 articles)\n",
        "  page += 1\n",
        "  if page < 11 and nav_buttons:\n",
        "    next_button = nav_buttons.find_all(\"a\").pop()\n",
        "    if next_button.text == \"Next\":\n",
        "      new_url = next_button.get(\"href\")\n",
        "      print(f\"Waiting five seconds before getting articles from page {page}\")\n",
        "      time.sleep(5)\n",
        "      return add_articles(new_url, page, articles)\n",
        "\n",
        "  return articles"
      ],
      "metadata": {
        "id": "DzhFgqvHfCrF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "articles = add_articles(base_url + date.strftime(\"%Y_%m_%d\"))"
      ],
      "metadata": {
        "id": "2eKCDH7hsyiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have links to 500 Yahoo Finance articles from March 10th. Let’s see where these links point to."
      ],
      "metadata": {
        "id": "rNbPF-UlHQ1j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.parse import urlparse\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    \"url\": [article.find(\"a\").get(\"href\") for article in articles],\n",
        "    \"headline\": [article.text for article in articles]\n",
        "})\n",
        "df[\"hostname\"] = df[\"url\"].apply(lambda x: urlparse(x).hostname)\n",
        "df[\"hostname\"].value_counts()"
      ],
      "metadata": {
        "id": "fMMxXQkmGvYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Super. In my initial testing, it looked like a lot of the links were to sites other than Yahoo Finance, which would cause a problem for scraping. Namely, every single website has a different way of presenting information, which means avid scrapers like us have to manage different ways of getting the information we want without a bunch of information we don’t want. \n",
        "\n",
        "Let’s toss out everything except articles that point to `finance.yahoo.com` and grab their text. \n",
        "\n",
        "Luckily, Yahoo Finance puts all of their article content in an `<article>` tag, so we don’t need to do much to grab the text."
      ],
      "metadata": {
        "id": "vVHRLITwKCuk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_yahoo_finance_article_text(url):\n",
        "  time.sleep(1)\n",
        "  print(url)\n",
        "  webpage = getWebPage(url)\n",
        "  try: \n",
        "    parsed_doc = BeautifulSoup(webpage, \"html.parser\")\n",
        "    if parsed_doc:\n",
        "      return parsed_doc.find(\"article\").prettify()\n",
        "    return np.nan\n",
        "  except:\n",
        "    return np.nan"
      ],
      "metadata": {
        "id": "22dN8J7cG_LD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### This code is illustrative. The second line will make hundreds of calls\n",
        "#   to Yahoo Finance, which is probably not what you want to do, especially\n",
        "#   especially since all of the articles are already downloaded and saved to\n",
        "#   our repository as a parquet file.\n",
        "\n",
        "# df = df[df[\"hostname\"] == \"finance.yahoo.com\"]\n",
        "# df[\"raw_html_text\"] = df[\"url\"].apply(lambda url: get_yahoo_finance_article_text(url))\n",
        "# df[\"raw_html_text\"].isna().sum() #-> 50\n",
        "# df = df.dropna()\n",
        "# df.to_parquet(\"mar_10_articles.parquet\")"
      ],
      "metadata": {
        "id": "NJ2r2EiBK95o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The process of getting all 393 articles obviously takes a while if we are waiting a second in between. Furthermore, there’s no real reason to go and download them all again for the purposes of this workshop, so instead, I drop all the ones that failed to grab an article for whatever reason (50 of 393) and convert the dataframe to a parquet so we can download it and keep working off it instead.\n",
        "\n",
        "This requires an extra little step because I save the raw HTML into the dataframe, not the parsed Beautiful Soup object, so when we import the parquet, we need to reparse everything.\n",
        "\n",
        "So let’s start from scratch with what we’ve got, because technically the scraping is over."
      ],
      "metadata": {
        "id": "JCEouMiuiybz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parsing Yahoo Finance Articles\n",
        "\n",
        "Let’s install a library and grab our articles so we can start doing some light textual analysis.\n"
      ],
      "metadata": {
        "id": "NydDuIwSikkD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install textacy\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "pGdMVA-DuO5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_parquet(\"https://github.com/columbia-data-club/meetings/blob/main/assets/data/mar_10_articles.parquet?raw=true\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "5wAJycFKiU22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Excellent! Let’s have a look at one of these articles to see what the basic structure is."
      ],
      "metadata": {
        "id": "cLS3bX9urh2x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample = df.sample(1, random_state=42)\n",
        "print(sample[\"url\"].iloc[0])\n",
        "with open('article.html', 'w') as file:\n",
        "    file.write(sample[\"raw_html_text\"].iloc[0])"
      ],
      "metadata": {
        "id": "Ffsr47iZrU9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here I kind of stop with the pre-fab code because I want to see how we all interpret this html and how we decide to go forward.\n",
        "\n",
        "We’ll be using the library [textaCy](https://textacy.readthedocs.io/), which has many preproccessing tools available for us, so we can start by naively extracting the text from our articles.\n",
        "\n",
        "**Usability note**: the textaCy that gets installed by default is above version 0.11.0 (0.12.0 as of this writing), but currently the documentation does not reflect the changes to the library, so things break. The [release notes for 0.12.0](https://github.com/chartbeat-labs/textacy/releases/tag/0.12.0) give some guidance."
      ],
      "metadata": {
        "id": "muyk-MDytu42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"naive_txt\"] = df[\"raw_html_text\"].apply(lambda x: BeautifulSoup(x, \"html.parser\").text)"
      ],
      "metadata": {
        "id": "gR7WX-7VrW6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df[\"naive_txt\"].sample(1, random_state=42).iloc[0])"
      ],
      "metadata": {
        "id": "Bklg42BPwexp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from textacy import preprocessing\n",
        "\n",
        "preproc = preprocessing.make_pipeline(\n",
        "    preprocessing.normalize.whitespace,\n",
        "    preprocessing.normalize.quotation_marks,\n",
        "    preprocessing.replace.emojis,\n",
        "    preprocessing.replace.emails\n",
        ")\n",
        "\n",
        "df[\"preproc_txt\"] = df[\"naive_txt\"].apply(preproc)"
      ],
      "metadata": {
        "id": "_pmy3X89wjw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df[\"preproc_txt\"].sample(1, random_state=42).iloc[0])"
      ],
      "metadata": {
        "id": "zY1P_HpnyUfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we’ve gotten this far, let’s convert everything to [spaCy docs](https://spacy.io/api/doc) using textaCy’s built in generator"
      ],
      "metadata": {
        "id": "0HGTqQ1l08wS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import textacy\n",
        "\n",
        "df[\"doc\"] = df[\"preproc_txt\"].apply(lambda x: textacy.make_spacy_doc(x, lang=\"en_core_web_sm\"))"
      ],
      "metadata": {
        "id": "BUq3vg-FyZJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can work through the textaCy [quick start](https://github.com/chartbeat-labs/textacy/blob/main/docs/source/quickstart.md) for 0.12.0, more or less, using the documentation on GitHub. Or do our own thing."
      ],
      "metadata": {
        "id": "rDWEesfl5iPB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = df[\"doc\"].sample(1, random_state=42).iloc[0]\n",
        "print(doc._.preview)"
      ],
      "metadata": {
        "id": "52nRbx261hO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(textacy.extract.entities(doc, exclude_types=\"NUMERIC\"))"
      ],
      "metadata": {
        "id": "XfrEOcscErFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from textacy import text_stats as ts\n",
        "print(ts.n_words(doc), ts.n_unique_words(doc))\n",
        "print(ts.diversity.ttr(doc))\n",
        "print(ts.flesch_kincaid_grade_level(doc))\n",
        "print(ts.counts.pos(doc))"
      ],
      "metadata": {
        "id": "cedxjp4h5Wem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xSyeTBY_54vD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}