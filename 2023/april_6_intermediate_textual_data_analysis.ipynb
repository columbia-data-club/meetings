{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/columbia-data-club/meetings/blob/main/2023/april_6_intermediate_textual_data_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHCAYxhpfZ0v"
      },
      "source": [
        "![A blue background with the SQLite logo and the words Data Club on it](https://raw.githubusercontent.com/columbia-data-club/meetings/main/assets/images/data-club-spacy.png)\n",
        "\n",
        "# Intermediate Textual Data Analysis\n",
        "\n",
        "April 6, 2023\n",
        "\n",
        "by [Moacir P. de Sá Pereira](https://moacir.com) for the [Columbia Data Club](https://github.com/columbia-data-club/).\n",
        "\n",
        "As we investigate our textual data in more detail, the techniques for analyzing such unstructured data rely on new libraries and models provided by machine learning. Here, we look to the cutting edge of contemporary Python text analysis libraries to learn how to mobilize their potential.\n",
        "\n",
        "This notebook builds on the notebook “[Exploratory Analysis of Textual Data](https://github.com/columbia-data-club/meetings/blob/main/2023/march_23_textual_analysis.ipynb)” from March 2023. It is a continuation of the work in that notebook, and, as such, takes the contents of that notebook for granted.\n",
        "\n",
        "We’ll start by installing the [Textacy](https://textacy.readthedocs.io/) library and [spaCy](https://spacy.io/) language models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U38a8chcfWWu"
      },
      "outputs": [],
      "source": [
        "!python -m pip install textacy\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3EUTx0ehWEg"
      },
      "source": [
        "## Isolating Text and Building Metadata\n",
        "\n",
        "In [the previous notebook](https://github.com/columbia-data-club/meetings/blob/main/2023/march_23_textual_analysis.ipynb), we limited ourselves to 393 articles published by Yahoo Finance on March 10, 2023. For this notebook, I’ve prepared a dataset that scraped every article published by Yahoo Finance on that date and then limited it to the articles from Yahoo Finance itself and the articles that did not yield errors of some sort."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPT056C4g2hz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_parquet(\"https://github.com/columbia-data-club/meetings/blob/main/assets/data/mar_10_articles_full.parquet?raw=true\",\n",
        "                     columns=[\"url\", \"headline\",\t\"hostname\",\t\"raw_html_text\"])\n",
        "print(len(df))\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvXEUBviiXER"
      },
      "source": [
        "We have 2504 total articles, then. When we captured the content of each article, we slurped up everything inside the `<article>` tag. This was generally a good idea at the time, but it includes things like the “Trending” sidebar, as well as other material ancillary to the text of the article itself.\n",
        "\n",
        "That said, some of that ancillary material can give us metadata for the articles that we could use in our analysis. The `<header>` tag, for example, gives the source of the article as well as the headline. Similarly, we can find the article’s author in the `<div class=\"caas-attr-item-author\">` as well as the estimated reading time in `<span class=\"caas-attr-mins-read\">`.\n",
        "\n",
        "The text of the news story resides entirely within `<p>` tags inside `<div class=\"caas-body\">`, so we can extract those paragraphs to rebuild the text without the additional textual elements inside the `<article>` tag. Time for more [Beautiful Soup](https://beautiful-soup-4.readthedocs.io/en/latest/).\n",
        "\n",
        "Then, let’s see where the articles come from and get a sense of the reading times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4DY-HK5IiNoM"
      },
      "outputs": [],
      "source": [
        "# This creates a single, sample article for inspecting\n",
        "\n",
        "# sample = df.sample(1, random_state=42)\n",
        "# sample_article = sample.iloc[0]\n",
        "# print(sample_article[\"url\"])\n",
        "# with open(\"article.html\", \"w\") as file:\n",
        "#     file.write(sample_article[\"raw_html_text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_I-d0D4p9zK"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "\n",
        "def extract_textual_features(raw_html_text):\n",
        "  html = BeautifulSoup(raw_html_text, \"html.parser\")\n",
        "  provider = html.find(\"span\", class_=\"caas-attr-provider\").text.strip()\n",
        "  publication_datetime = datetime.strptime(html.find(\"time\").get(\"datetime\"), \"%Y-%m-%dT%H:%M:%S.%f%z\")\n",
        "  byline = html.find(\"div\", class_=\"caas-attr-item-author\").text.strip()\n",
        "  read_time = np.nan\n",
        "  read_time_span = html.find(\"span\", class_=\"caas-attr-mins-read\")\n",
        "  if read_time_span:\n",
        "    read_time = int(read_time_span.text.strip().replace(\" min read\", \"\"), base=10)\n",
        "  text = \" \".join([p.text.strip() for p in html.find(\"div\", class_=\"caas-body\").find_all(\"p\")])\n",
        "  char_count = len(text)\n",
        "\n",
        "  return [provider, publication_datetime, byline, read_time, char_count, text]\n",
        "\n",
        "# print(extract_textual_features(sample_article[\"raw_html_text\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JkD-eTFWrEVE"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "tqdm.pandas()\n",
        "\n",
        "# ~1:13 to complete\n",
        "df[[\"provider\", \"publication_datetime\", \"byline\", \"read_time\", \"char_count\", \"text\"]] = df.progress_apply(\n",
        "    lambda row: extract_textual_features(row[\"raw_html_text\"]),\n",
        "    axis=1,\n",
        "    result_type=\"expand\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGJFCRuUtBOI"
      },
      "outputs": [],
      "source": [
        "ax = df[\"provider\"].value_counts().plot(\n",
        "    kind=\"bar\", \n",
        "    figsize=(10,2),\n",
        "    title=\"Article Provider Distribution on March 10, 2023\")\n",
        "ax.set_xlabel(\"Provider\")\n",
        "ax.set_ylabel(\"Count\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GRMTlQmJ7lZ"
      },
      "outputs": [],
      "source": [
        "df[\"read_time\"].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5kJgEJRIP0a"
      },
      "source": [
        "## Making Docs with spaCy and a Corpus with Textacy\n",
        "\n",
        "Excellent work. With this metadata split out and the article text isolated, we can move forward and do the computationally heavy part of the workbook, converting each article into a spaCy [`Doc`](https://spacy.io/api/doc). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vahQUpIUyLsi"
      },
      "outputs": [],
      "source": [
        "import textacy\n",
        "from textacy import preprocessing\n",
        "\n",
        "preproc = preprocessing.make_pipeline(\n",
        "    preprocessing.normalize.whitespace,\n",
        "    preprocessing.normalize.quotation_marks,\n",
        "    preprocessing.replace.emojis,\n",
        "    preprocessing.replace.emails,\n",
        "    preprocessing.replace.urls\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EekKKmumPPkk"
      },
      "outputs": [],
      "source": [
        "def build_doc_from_row(row):\n",
        "  metadata = {\n",
        "      \"title\": row[\"headline\"],\n",
        "      \"url\": row[\"url\"],\n",
        "      \"provider\": row[\"provider\"],\n",
        "      \"byline\": row[\"byline\"],\n",
        "      \"read_time\": row[\"read_time\"],\n",
        "      \"char_count\": row[\"char_count\"],\n",
        "      \"publication_datetime\": row[\"publication_datetime\"].strftime(\"%Y-%m-%dT%H:%M:%S.%f%z\")\n",
        "  }\n",
        "  \n",
        "  return textacy.make_spacy_doc((row[\"text\"], metadata), lang=\"en_core_web_sm\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pW6BXpPVKLms"
      },
      "outputs": [],
      "source": [
        "# ~ 7:00\n",
        "\n",
        "df[\"doc\"] = df.progress_apply(lambda row: build_doc_from_row(row), axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oR3-eYPU9lf"
      },
      "source": [
        "Now that we have created all the `Doc`s, we can abandon `pandas` and collapse our dataset into a Textacy `Corpus`, which well let us work with the entire collection at once, instead of iteratively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvEtXZD3POAY"
      },
      "outputs": [],
      "source": [
        "corpus = textacy.Corpus(\"en_core_web_sm\", data=df[\"doc\"])\n",
        "print(corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkOvCR-7V_Ic"
      },
      "source": [
        "And what’s more, we can save the corpus to disk. We have not hit the computer too hard so far, but this is a good, safe step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8O99y204VzhL"
      },
      "outputs": [],
      "source": [
        "corpus.save(\"corpus.bin.gz\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYzYd0_TW7Qu"
      },
      "source": [
        "Surprisingly, at 23Mb, the corpus takes up about half the disk space as the parquet file with the raw HTML of all the articles (42Mb). However, if we also gzip the parquet file, its size falls to 3.9Mb. Hopefully this gives a sense both of how useful gzip is for moving files around quickly and how much data has been added since we tokenized and parsed the text with spaCy to make those `Doc`s.\n",
        "\n",
        "Anyway, we can load up the corpus from disk like this with our old friend, the Requests library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0v-apXKVWLv7"
      },
      "outputs": [],
      "source": [
        "import textacy # In case we're starting from this cell.\n",
        "import requests\n",
        "\n",
        "corpus_response = requests.get(\n",
        "    \"https://github.com/columbia-data-club/meetings/blob/main/assets/data/mar_10_articles_corpus.bin.gz?raw=true\"\n",
        ")\n",
        "with open(\"mar_10_articles_corpus.bin.gz\",\"wb\") as f:\n",
        "  f.write(corpus_response.content)\n",
        "\n",
        "corpus = textacy.Corpus.load(\"en_core_web_sm\", \"./mar_10_articles_corpus.bin.gz\")\n",
        "print(corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooqzH_xR2Gbi"
      },
      "source": [
        "Textacy includes some properties that let us get some information about our corpus as a whole, and we’ll look at those now. Here, we’ll be using some of the tricks from the [Textacy `Corpus` tutorial](https://textacy.readthedocs.io/en/latest/tutorials/tutorial-2.html). Textacy’s topic modeling is just an extension of [scikit-learn’s decomposition models](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.decomposition), and we’ll be using the one for [latent Dirichlet allocation](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html).\n",
        "\n",
        "We’ll:\n",
        "\n",
        "1. Extract named entities.\n",
        "2. Lemmatize our `Doc`s, which bundles morphologically different versions of words into one (“was” and “is” both get filed under “are”).\n",
        "3. Vectorize the documents, which lets us compare them to each other and measure distinctness of documents, creating a matrix with documents along one axis and terms along the other.\n",
        "4. Generate some topic models based on the categorical distinctions of the articles. \n",
        "5. Visualize the topics.\n",
        "\n",
        "Before continuing, I want to emphasize the compex nature of topic modeling and underscore that these topics are not necessarily of particular semantic use. Topic modeling works by splitting $n$ documents into $m$ topics, but the topics are not what the articles are “about.” “Topic” is a bit of a misleading term here, since we associate it with a summary or description of the information a chunk of text passes to the reader. That is not what happens in topic modeling. So if a topic appears with, say aquatic terms in it, that does not mean that the documents in that topic are about the ocean. It simply means that they used those words that seemed commmon enough to be one of the $m$ topics, but distinct enough so they only applied to a specific subset of documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-KS79fYZdNC"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from textacy import extract\n",
        "# Partial takes a function as its first positional argument and passes the rest\n",
        "from functools import partial\n",
        "\n",
        "def extract_terms(doc):\n",
        "  return list(extract.terms(\n",
        "    doc,\n",
        "    # n-gram terms and settings\n",
        "    ngs=partial(extract.ngrams, n=2, include_pos={\"NOUN\", \"ADJ\"}),\n",
        "    # etntity terms and settings. GPE = Geopolitical entity. Try `spacy.explain(\"GPE\")`\n",
        "    ents=partial(extract.entities, include_types={\"PERSON\", \"ORG\", \"GPE\", \"LOC\"}),\n",
        "    # dont extract extract entities if they are already n-grams\n",
        "    dedupe=True\n",
        "    ))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_article = random.choice(corpus)\n",
        "print(sample_article._.meta[\"url\"])\n",
        "terms = extract_terms(sample_article)\n",
        "terms"
      ],
      "metadata": {
        "id": "ry-FZKML79PS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8sPk8l12ZzKE"
      },
      "outputs": [],
      "source": [
        "list(extract.terms_to_strings(terms, by=\"lemma\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lHZRzPblD7o_"
      },
      "outputs": [],
      "source": [
        "docs_terms = (extract_terms(doc) for doc in tqdm(corpus))\n",
        "tokenized_docs = (extract.terms_to_strings(doc_terms, by=\"lemma\") for doc_terms in docs_terms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9POkRE2DYzD"
      },
      "outputs": [],
      "source": [
        "from textacy import representations\n",
        "\n",
        "doc_term_matrix, vocab = representations.build_doc_term_matrix(tokenized_docs,\n",
        "  # minimum document frequency, as a percentage ([0, 1]) or fixed number\n",
        "  min_df = 5,\n",
        "  # maximum document frequency\n",
        "  max_df = 0.7,\n",
        "  # how to weight term frequency\n",
        "  tf_type=\"linear\",\n",
        "  # which idf equation to use.\n",
        "  # this is idf = log(n_docs + 1 / df + 1) + 1 \n",
        "  idf_type=\"smooth\"\n",
        ")\n",
        "doc_term_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHMDFX1kFGnV"
      },
      "outputs": [],
      "source": [
        "import textacy.tm\n",
        "\n",
        "model = textacy.tm.TopicModel(\"lda\", n_topics=10)\n",
        "model.fit(doc_term_matrix)\n",
        "doc_topic_matrix = model.transform(doc_term_matrix)\n",
        "doc_topic_matrix.shape # Should be (2340, 10) as we asked for 10 topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_jTHSiXFlkC"
      },
      "outputs": [],
      "source": [
        "doc_topic_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRHUmcvfFoxm"
      },
      "outputs": [],
      "source": [
        "id_to_term = {id_: term for term, id_ in vocab.items()}\n",
        "for topic_idx, terms in model.top_topic_terms(id_to_term, top_n=8):\n",
        "  print(f\"topic {topic_idx}: {'  '.join(terms)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHY9kkL6PviL"
      },
      "outputs": [],
      "source": [
        "for topic_idx, doc_idxs in model.top_topic_docs(doc_topic_matrix, top_n=8):\n",
        "  print(f\"topic {topic_idx}: {'   '.join(corpus[doc_idx]._.meta['provider'] for doc_idx in doc_idxs)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_ = model.termite_plot(doc_term_matrix, id_to_term, n_terms=30)\n"
      ],
      "metadata": {
        "id": "GlQ5l_t_vdHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What can we tell from these descriptions of the ten topics we asked for? What sorts of changes might we want to implement if we wanted “better” topics? "
      ],
      "metadata": {
        "id": "eUBwP3gk3ufD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let’s Do This again with Gensim\n",
        "\n",
        "I was considering doing more individual, `Doc`-level analysis with spaCy, but I started wondering about how good our topic models were, and if there were a way to test this. In what proceeds, I’ll be adapting from the topic modeling blueprints in [_Blueprints for Text Analytics in Python_](https://www.oreilly.com/library/view/blueprints-for-text/9781492074076/), a 2020 book written by Jens Albrecht, Sidharth Ramachandran, and Christian Winkler. In looking over their code, we can see how radically different their approach is, and not just because they are using [Gensim](https://radimrehurek.com/gensim/index.html) instead of Textacy/scikit-learn."
      ],
      "metadata": {
        "id": "NsKOpqarQcK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models import TfidfModel\n",
        "from gensim.models.nmf import Nmf\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stopwords = set(nltk.corpus.stopwords.words(\"english\"))"
      ],
      "metadata": {
        "id": "YNuBu027v57T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gensim_docs = [[w for w in re.findall(r'\\b\\w\\w+\\b', paragraph.lower()) if w not in stopwords] for paragraph in df[\"text\"]]\n",
        "dict_gensim_docs = Dictionary(gensim_docs)\n",
        "dict_gensim_docs.filter_extremes(no_below=5, no_above=0.7)\n",
        "bow_gensim_docs = [dict_gensim_docs.doc2bow(doc) for doc in gensim_docs]\n",
        "tfidf_gensim_docs = TfidfModel(bow_gensim_docs)\n",
        "vectors_gensim_docs = tfidf_gensim_docs[bow_gensim_docs]"
      ],
      "metadata": {
        "id": "KK-ytplzhiR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nmf_gensim_docs = Nmf(vectors_gensim_docs, \n",
        "  num_topics=10, \n",
        "  id2word=dict_gensim_docs, \n",
        "  kappa=0.1, eval_every=5)\n"
      ],
      "metadata": {
        "id": "NB8zWjLZig68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for (topic_idx, terms) in nmf_gensim_docs.show_topics(formatted=False):\n",
        "  print(f\"topic {topic_idx}: {'   '.join([term[0] for term in terms])}\")"
      ],
      "metadata": {
        "id": "yr9xBlDkolD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What do we think?"
      ],
      "metadata": {
        "id": "LWyHg207rlNp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "\n",
        "nmf_gensim_docs_coherence = CoherenceModel(model=nmf_gensim_docs, \n",
        "  texts=gensim_docs,\n",
        "  dictionary=dict_gensim_docs,\n",
        "  coherence=\"c_v\")\n",
        "print(nmf_gensim_docs_coherence.get_coherence())"
      ],
      "metadata": {
        "id": "vDVDa7jbqMX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we moved to Gensim, we made a few other changes, like using [non-negative matrix factorization](https://en.wikipedia.org/wiki/Non-negative_matrix_factorization) as our model algorithm. Scikit-learn also provides NMF, but I had some trouble using it in testing this notebook and switched to LDA in a panic. Nevertheless, we can compare our Gensim NMF model to a Gensim LDA model using the same corpus. "
      ],
      "metadata": {
        "id": "5mxqMBX_s0N4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import LdaModel\n",
        "\n",
        "lda_gensim_docs = LdaModel(corpus=bow_gensim_docs,\n",
        "  id2word=dict_gensim_docs,\n",
        "  chunksize=2000,\n",
        "  alpha=\"auto\",\n",
        "  eta=\"auto\",\n",
        "  iterations=400,\n",
        "  num_topics=10,\n",
        "  passes=20,\n",
        "  eval_every=None,\n",
        "  random_state=42)\n",
        "for (topic_idx, terms) in lda_gensim_docs.show_topics(formatted=False):\n",
        "  print(f\"topic {topic_idx}: {'   '.join([term[0] for term in terms])}\")"
      ],
      "metadata": {
        "id": "Q7XNwjpDslPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lda_gensim_docs_coherence = CoherenceModel(model=lda_gensim_docs, \n",
        "  texts=gensim_docs,\n",
        "  dictionary=dict_gensim_docs,\n",
        "  coherence=\"c_v\")\n",
        "print(lda_gensim_docs_coherence.get_coherence())"
      ],
      "metadata": {
        "id": "X6WNeJT5vSin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The coherence score for the probablistic LDA model is higher than the decompositional NMF model. This is surprising because my understanding is that NMF is considered to provide more specific topics and topics that are more closely related to semantic topics (as in, actually telling us what documents are _about_).\n",
        "\n",
        "The O’Reilly book includes some code for predicting the “correct” number of topics for a corpus. Unfortunately, calculating coherence takes a long time (this analysis takes over an hour), so here I move to a different notebook to see the results.\n",
        "\n",
        "In closing, let’s try NMF with Textacy again."
      ],
      "metadata": {
        "id": "MIKQgwlowN-l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Back to Textacy"
      ],
      "metadata": {
        "id": "p_3Q0V-IA7LU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nmf_model = textacy.tm.TopicModel(\"nmf\", n_topics=10)\n",
        "nmf_model.fit(doc_term_matrix)\n",
        "nmf_doc_topic_matrix = nmf_model.transform(doc_term_matrix)\n",
        "\n",
        "print(\"--- LDA ---\")\n",
        "\n",
        "for topic_idx, terms in model.top_topic_terms(id_to_term, top_n=8):\n",
        "  print(f\"topic {topic_idx}: {'  '.join(terms)}\")\n",
        "\n",
        "print(\"--- NMF ---\")\n",
        "\n",
        "for topic_idx, terms in nmf_model.top_topic_terms(id_to_term, top_n=8):\n",
        "  print(f\"topic {topic_idx}: {'  '.join(terms)}\")\n"
      ],
      "metadata": {
        "id": "6g_JYMWd4puh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_2 = nmf_model.termite_plot(doc_term_matrix, id_to_term, n_terms=30)\n"
      ],
      "metadata": {
        "id": "QE9_5Cse8J9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lda_matrix = pd.DataFrame(doc_topic_matrix)\n",
        "nmf_matrix = pd.DataFrame(nmf_doc_topic_matrix)"
      ],
      "metadata": {
        "id": "gN0oDG7A9MP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lda_matrix.describe()"
      ],
      "metadata": {
        "id": "dCDlcRCx-_tX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nmf_matrix.describe()"
      ],
      "metadata": {
        "id": "LbesDWVX_yFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "opsaV5HKAHu3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNiCfApaDv1JETsIrY7Nv+H",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}