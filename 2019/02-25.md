# Second meeting

## Xiaoxi presented on her Pokémon classification project.

We learned about linear and tree models:

* [Logistic Regression](https://machinelearningmastery.com/logistic-regression-for-machine-learning/) 
* [CART](https://en.wikipedia.org/wiki/Decision_tree_learning)

In classification tests, tree models will typically outperform linear models.

We also learned about ensemble models, that combine models together.

* [Random Forest](https://en.wikipedia.org/wiki/Random_forest)
* [Boosting](https://en.wikipedia.org/wiki/Gradient_boosting#Gradient_tree_boosting)
* [XGBoost](https://xgboost.readthedocs.io/en/latest/)

In the Pokémon project, boosting gave the highest accuracy in picking
predicting legendary Pokémon (95%), but it was [KNN](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) that gave the best area
under the curve (sensitivity against specificity).

## How to figure out what kind of classification to use?

Numerical data works well with these models, but textual data will not work as
well. For that, something like deep learning is more useful. 

## Next week

Intro to TensorFlow by Kang.
